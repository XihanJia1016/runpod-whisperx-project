"""
RunPod WhisperX Large-v3 é«˜ç²¾åº¦éŸ³é¢‘å¤„ç†è„šæœ¬
åŠè‡ªåŠ¨ç§å­è¯†åˆ«è¯´è¯äººæ—¥å¿—ç³»ç»Ÿ
ä¼˜åŒ–è¯´è¯äººè¯†åˆ«å’Œæ—¶é—´æˆ³ç²¾åº¦
"""

import whisperx
import pandas as pd
import torch
import gc
import os
import time
import logging
from pathlib import Path
import numpy as np
from pyannote.audio import Pipeline
import subprocess
import shutil
import difflib
from sklearn.metrics.pairwise import cosine_similarity

# ç¯å¢ƒæ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤
def check_and_fix_environment():
    """æ£€æŸ¥ç¯å¢ƒå¹¶åœ¨éœ€è¦æ—¶è‡ªåŠ¨ä¿®å¤"""
    
    missing_packages = []
    version_conflicts = []
    
    try:
        import torch
        import torchvision
        
        torch_version = torch.__version__
        torchvision_version = torchvision.__version__
        
        # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
        if not (torch_version.startswith('2.1.') and torchvision_version.startswith('0.16.')):
            version_conflicts.append(f"PyTorchç‰ˆæœ¬å†²çª: torch={torch_version}, torchvision={torchvision_version}")
    except ImportError as e:
        missing_packages.append(f"PyTorch: {e}")
    
    try:
        import pyannote.audio
    except ImportError as e:
        missing_packages.append(f"pyannote.audio: {e}")
    
    try:
        import whisperx
    except ImportError as e:
        missing_packages.append(f"whisperx: {e}")
    
    try:
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError as e:
        missing_packages.append(f"scikit-learn: {e}")
    
    if missing_packages or version_conflicts:
        print("âŒ ç¯å¢ƒæ£€æŸ¥å‘ç°é—®é¢˜:")
        for issue in missing_packages + version_conflicts:
            print(f"  - {issue}")
        
        print("\nğŸ”§ è‡ªåŠ¨ä¿®å¤å»ºè®®:")
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿®å¤ç¯å¢ƒ:")
        print("  python setup_environment.py")
        print("\næˆ–è€…æ‰‹åŠ¨ä¿®å¤:")
        print("  pip install -r requirements_stable.txt")
        
        # è¯¢é—®æ˜¯å¦è‡ªåŠ¨ä¿®å¤
        response = input("\næ˜¯å¦ç°åœ¨è‡ªåŠ¨ä¿®å¤ï¼Ÿ(y/N): ")
        if response.lower() in ['y', 'yes']:
            print("ğŸ”„ å¼€å§‹è‡ªåŠ¨ä¿®å¤...")
            try:
                import setup_environment
                setup_environment.main()
                print("âœ… ç¯å¢ƒä¿®å¤å®Œæˆï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬")
                exit(0)
            except Exception as e:
                print(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨è¿è¡Œ: python setup_environment.py")
                exit(1)
        else:
            print("è¯·å…ˆä¿®å¤ç¯å¢ƒé—®é¢˜åå†è¿è¡Œè„šæœ¬")
            exit(1)
    else:
        print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")

# ç§»é™¤è‡ªåŠ¨ç¯å¢ƒæ£€æŸ¥ï¼Œé¿å…åœ¨RunPodä¸­å¡ä½
# check_and_fix_environment()  # å·²ç¦ç”¨ï¼Œé˜²æ­¢äº¤äº’å¼è¾“å…¥å¯¼è‡´å¡æ­»

# ç¦ç”¨TF32ä»¥é¿å…ç²¾åº¦å’Œå…¼å®¹æ€§é—®é¢˜
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_cudnn_issue():
    """ä¿®å¤cuDNNåº“é—®é¢˜"""
    logger.info("ğŸ”§ å°è¯•ä¿®å¤cuDNNåº“é—®é¢˜...")
    
    try:
        # å°è¯•å®‰è£…ç¼ºå¤±çš„cuDNNåº“
        logger.info("ğŸ”§ å®‰è£…cuDNNåº“...")
        subprocess.run(['apt', 'update'], check=True, capture_output=True)
        subprocess.run(['apt', 'install', '-y', 'libcudnn8', 'libcudnn8-dev'], check=True, capture_output=True)
        logger.info("âœ… cuDNNåº“å®‰è£…å®Œæˆ")
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        logger.warning(f"âš ï¸ æ— æ³•é€šè¿‡aptå®‰è£…cuDNN: {e}")
        
        # å°è¯•ä½¿ç”¨condaå®‰è£…
        try:
            logger.info("ğŸ”§ å°è¯•é€šè¿‡condaå®‰è£…cudnn...")
            subprocess.run(['conda', 'install', '-c', 'conda-forge', 'cudnn', '-y'], check=True, capture_output=True)
            logger.info("âœ… cuDNNé€šè¿‡condaå®‰è£…å®Œæˆ")
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.warning(f"âš ï¸ condaå®‰è£…ä¹Ÿå¤±è´¥: {e}")

def setup_cuda_environment():
    """è®¾ç½®CUDAç¯å¢ƒå˜é‡ä¿®å¤cuDNNé—®é¢˜"""
    # å…ˆå°è¯•ä¿®å¤cuDNN
    fix_cudnn_issue()
    
    cuda_paths = [
        "/usr/local/cuda/lib64",
        "/usr/local/cuda-11.8/lib64", 
        "/usr/local/cuda-11.7/lib64",
        "/usr/local/cuda-11.6/lib64",
        "/usr/lib/x86_64-linux-gnu",
        "/opt/conda/lib",
        "/opt/conda/pkgs/cudnn*/lib",
        "/usr/local/lib/python3.10/dist-packages/nvidia/cudnn/lib"
    ]
    
    # æ£€æŸ¥PyTorchä¿¡æ¯
    logger.info(f"ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"ğŸ”§ cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # è®¾ç½®LD_LIBRARY_PATH
    current_path = os.environ.get('LD_LIBRARY_PATH', '')
    new_paths = []
    
    for path in cuda_paths:
        if os.path.exists(path):
            new_paths.append(path)
            logger.info(f"âœ… æ‰¾åˆ°CUDAåº“è·¯å¾„: {path}")
    
    if new_paths:
        if current_path:
            os.environ['LD_LIBRARY_PATH'] = ':'.join(new_paths) + ':' + current_path
        else:
            os.environ['LD_LIBRARY_PATH'] = ':'.join(new_paths)
        logger.info(f"ğŸ”§ å·²è®¾ç½®LD_LIBRARY_PATH")
    
    # è®¾ç½®é¢å¤–çš„ç¯å¢ƒå˜é‡æ¥ç¨³å®šcuDNN
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    # å¼ºåˆ¶é‡æ–°åŠ è½½åŠ¨æ€åº“
    try:
        import ctypes
        ctypes.CDLL("libcudnn.so.8", mode=ctypes.RTLD_GLOBAL)
        logger.info("âœ… æˆåŠŸåŠ è½½libcudnn.so.8")
    except Exception as e:
        logger.warning(f"âš ï¸ æ— æ³•åŠ è½½libcudnn.so.8: {e}")
    
def ensure_ffmpeg():
    """ç¡®ä¿ffmpegå·²å®‰è£…"""
    if shutil.which('ffmpeg') is None:
        logger.info("ğŸ”§ æœªæ‰¾åˆ°ffmpegï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
        try:
            # å°è¯•ä½¿ç”¨aptå®‰è£…
            subprocess.run(['apt', 'update'], check=True, capture_output=True)
            subprocess.run(['apt', 'install', '-y', 'ffmpeg'], check=True, capture_output=True)
            logger.info("âœ… ffmpegå®‰è£…æˆåŠŸ")
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                # å¦‚æœaptå¤±è´¥ï¼Œå°è¯•conda
                subprocess.run(['conda', 'install', '-c', 'conda-forge', 'ffmpeg', '-y'], check=True, capture_output=True)
                logger.info("âœ… ffmpegé€šè¿‡condaå®‰è£…æˆåŠŸ")
            except (subprocess.CalledProcessError, FileNotFoundError):
                logger.error("âŒ æ— æ³•è‡ªåŠ¨å®‰è£…ffmpegï¼Œè¯·æ‰‹åŠ¨å®‰è£…")
                logger.info("ğŸ’¡ æ‰‹åŠ¨å®‰è£…å‘½ä»¤: apt install -y ffmpeg æˆ– conda install -c conda-forge ffmpeg")
                raise RuntimeError("ffmpegæœªå®‰è£…ä¸”æ— æ³•è‡ªåŠ¨å®‰è£…")
    else:
        logger.info("âœ… ffmpegå·²å®‰è£…")

class HighPrecisionAudioProcessor:
    def __init__(self):
        # è®¾ç½®CUDAç¯å¢ƒä¿®å¤cuDNNé—®é¢˜
        setup_cuda_environment()
        # ç¡®ä¿ffmpegå·²å®‰è£…
        ensure_ffmpeg()
        self.device = self._setup_device()
        self.model = None
        self.align_model = None
        self.metadata = None
        self.diarize_model = None
        self.embedding_model = None
        
        # é«˜ç²¾åº¦é…ç½®
        self.config = {
            'model_size': 'large-v3',
            'batch_size': 8,  # GPUä¼˜åŒ–
            'chunk_length': 30,  # 30ç§’å—ï¼Œå¹³è¡¡ç²¾åº¦å’Œå†…å­˜
            'return_attention': True,
            'word_timestamps': True,
            'vad_filter': True,
            'temperature': 0.0,  # ç¡®å®šæ€§è¾“å‡º
            'enable_speaker_diarization': True,  # å¯ç”¨è¯´è¯äººè¯†åˆ«
        }
        
        logger.info(f"åˆå§‹åŒ–é«˜ç²¾åº¦å¤„ç†å™¨: è®¾å¤‡={self.device}")
    
    def _setup_device(self):
        if torch.cuda.is_available():
            device = "cuda"
            logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            device = "cpu"
            logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        return device
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰å¿…éœ€æ¨¡å‹"""
        try:
            # 1. åŠ è½½Whisper Large-v3
            logger.info("åŠ è½½Whisper Large-v3æ¨¡å‹...")
            self.model = whisperx.load_model(
                self.config['model_size'], 
                device=self.device,
                compute_type="float16",
                download_root="/workspace/cache"
            )
            logger.info("âœ… Whisper Large-v3åŠ è½½å®Œæˆ")
            
            # 2. åŠ è½½å¯¹é½æ¨¡å‹ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            logger.info("åŠ è½½å¼ºåˆ¶å¯¹é½æ¨¡å‹...")
            try:
                self.align_model, self.metadata = whisperx.load_align_model(
                    language_code="nl",  # è·å…°è¯­
                    device=self.device
                )
                logger.info("âœ… å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                logger.warning(f"è·å…°è¯­å¯¹é½æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.info("å°è¯•ä½¿ç”¨è‹±è¯­å¯¹é½æ¨¡å‹ä½œä¸ºå¤‡é€‰...")
                try:
                    self.align_model, self.metadata = whisperx.load_align_model(
                        language_code="en",  # è‹±è¯­ä½œä¸ºå¤‡é€‰
                        device=self.device
                    )
                    logger.info("âœ… è‹±è¯­å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰")
                except Exception as e2:
                    logger.warning(f"å¯¹é½æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: {e2}")
                    logger.info("å°†è·³è¿‡å¯¹é½æ­¥éª¤ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´æˆ³")
                    self.align_model = None
                    self.metadata = None
            
            # 3. è¯´è¯äººåµŒå…¥æ¨¡å‹å°†é‡‡ç”¨"ç”¨æ—¶åŠ è½½ï¼Œç”¨å®Œå³æ¯"ç­–ç•¥
            logger.info("âœ… è¯´è¯äººåµŒå…¥æ¨¡å‹å°†åŠ¨æ€åŠ è½½ï¼ˆç”¨æ—¶åŠ è½½ï¼Œç”¨å®Œå³æ¯ï¼‰")
            
            # éªŒè¯HuggingFace token
            hf_token = os.getenv('HF_TOKEN')
            if not hf_token:
                logger.warning("âš ï¸ æœªè®¾ç½®HF_TOKENç¯å¢ƒå˜é‡ï¼Œè¯´è¯äººåµŒå…¥å¯èƒ½å¤±è´¥")
                logger.info("ğŸ’¡ è¯·è¿è¡Œ: export HF_TOKEN='your_token_here'")
            else:
                logger.info(f"ğŸ”‘ HF_TOKENå·²è®¾ç½®: {hf_token[:20]}...")
            
            # ä¸å†é¢„åŠ è½½embeddingæ¨¡å‹ï¼Œæ”¹ä¸ºåŠ¨æ€åŠ è½½
            self.embedding_model = None
            self.diarize_model = None
            
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _seconds_to_timestamp(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _clean_text_for_comparison(self, text):
        """æ¸…ç†æ–‡æœ¬ä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œæ¯”è¾ƒ"""
        if pd.isna(text):
            return ""
        import re
        return re.sub(r'\s+', ' ', str(text).strip().lower())
    
    def _force_align_initial_turns(self, golden_turns_df, all_segments, num_turns=3):
        """
        é˜¶æ®µä¸€ï¼šå¼ºåˆ¶åˆ†é…å¼€åœºè½®æ¬¡çš„è¯´è¯äºº
        
        Args:
            golden_turns_df: é»„é‡‘æ–‡æœ¬DataFrame
            all_segments: æ‰€æœ‰AIè½¬å½•ç‰‡æ®µåˆ—è¡¨
            num_turns: å¼ºåˆ¶åˆ†é…çš„è½®æ¬¡æ•°ï¼Œé»˜è®¤3è½®
            
        Returns:
            tuple: (processed_segments, remaining_segments, success)
                - processed_segments: å·²å¼ºåˆ¶åˆ†é…å¥½è¯´è¯äººçš„ç‰‡æ®µåˆ—è¡¨
                - remaining_segments: å‰©ä½™æœªå¤„ç†çš„ç‰‡æ®µåˆ—è¡¨  
                - success: æ­¤é˜¶æ®µæ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ¯ å¼ºåˆ¶åˆ†é…å‰{num_turns}è½®è¯´è¯äºº...")
        
        processed_segments = []
        cursor = 0  # AIç‰‡æ®µçš„æŒ‡é’ˆ
        success = True
        
        try:
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è½®æ¬¡å¯ä»¥å¤„ç†
            actual_turns = min(num_turns, len(golden_turns_df))
            logger.info(f"å®é™…å¤„ç†è½®æ¬¡: {actual_turns}")
            
            # å¾ªç¯å¤„ç†æ¯ä¸€è½®
            for i in range(actual_turns):
                golden_turn = golden_turns_df.iloc[i]
                speaker = golden_turn['role']
                text = golden_turn['text']
                
                logger.info(f"å¤„ç†ç¬¬{i+1}è½® - è¯´è¯äºº: {speaker}")
                
                # ä»å½“å‰cursorä½ç½®å¼€å§‹å¯»æ‰¾åŒ¹é…çš„AIç‰‡æ®µ
                available_segments = all_segments[cursor:]
                
                if not available_segments:
                    logger.warning(f"ç¬¬{i+1}è½®: æ²¡æœ‰å‰©ä½™çš„AIç‰‡æ®µå¯åŒ¹é…")
                    success = False
                    break
                
                # ä½¿ç”¨ç°æœ‰çš„è´ªå¿ƒå¯¹é½é€»è¾‘æ‰¾åˆ°æœ€åŒ¹é…çš„ç‰‡æ®µç»„åˆ
                matched_segments = self._find_best_matching_segments(text, available_segments)
                
                if not matched_segments:
                    logger.warning(f"ç¬¬{i+1}è½®: æœªæ‰¾åˆ°åŒ¹é…çš„AIç‰‡æ®µ")
                    success = False
                    break
                
                # å¼ºåˆ¶åˆ†é…è¯´è¯äºº
                for segment in matched_segments:
                    segment['speaker'] = speaker
                    segment['confidence'] = 1.0  # åŸºäºé»„é‡‘æ ‡å‡†ï¼Œç½®ä¿¡åº¦æœ€é«˜
                
                # æ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨
                processed_segments.extend(matched_segments)
                
                # æ›´æ–°cursoråˆ°æœ€åä¸€ä¸ªåŒ¹é…ç‰‡æ®µçš„ä¸‹ä¸€ä¸ªä½ç½®
                last_matched_idx = None
                for j, seg in enumerate(all_segments):
                    if seg in matched_segments:
                        last_matched_idx = j
                
                if last_matched_idx is not None:
                    cursor = last_matched_idx + 1
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°ç´¢å¼•ï¼Œä¿å®ˆåœ°åªç§»åŠ¨1ä½
                    cursor += len(matched_segments)
                
                logger.info(f"âœ… ç¬¬{i+1}è½®å®Œæˆ: åˆ†é…{len(matched_segments)}ä¸ªç‰‡æ®µç»™{speaker}, cursorç§»è‡³{cursor}")
            
            # è®¡ç®—å‰©ä½™ç‰‡æ®µ
            remaining_segments = all_segments[cursor:] if cursor < len(all_segments) else []
            
            logger.info(f"ğŸ¯ å¼ºåˆ¶åˆ†é…é˜¶æ®µå®Œæˆ: å¤„ç†äº†{len(processed_segments)}ä¸ªç‰‡æ®µ, å‰©ä½™{len(remaining_segments)}ä¸ªç‰‡æ®µ")
            return processed_segments, remaining_segments, success
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶åˆ†é…é˜¶æ®µå¤±è´¥: {e}")
            return [], all_segments, False

    def _find_seed_segments(self, golden_turns_df, ai_segments):
        """
        ä½¿ç”¨è´¨é‡ä¼˜å…ˆç­–ç•¥æ ¹æ®é»„é‡‘æ–‡æœ¬æ‰¾åˆ°æ¯ä¸ªè¯´è¯äººçš„ç§å­ç‰‡æ®µ
        
        Args:
            golden_turns_df: å·²ç­›é€‰çš„é»„é‡‘æ–‡æœ¬DataFrame (å½“å‰dyadå’Œconversation)
            ai_segments: å½“å‰å¯¹è¯çš„æ‰€æœ‰AIè½¬å½•ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            dict: {'S': [segment1, segment2, ...], 'L': [segment3, segment4, ...]}
        """
        logger.info("å¼€å§‹å¯»æ‰¾è¯´è¯äººç§å­ç‰‡æ®µï¼ˆè´¨é‡ä¼˜å…ˆç­–ç•¥ï¼‰...")
        
        seed_map = {'S': [], 'L': []}
        
        try:
            # æ‰¾åˆ°Så’ŒLçš„è½®æ¬¡
            s_turns = golden_turns_df[golden_turns_df['role'] == 'S']
            l_turns = golden_turns_df[golden_turns_df['role'] == 'L']
            
            if s_turns.empty or l_turns.empty:
                logger.warning("æœªæ‰¾åˆ°Sæˆ–Lçš„é»„é‡‘æ–‡æœ¬ï¼Œæ— æ³•ç”Ÿæˆç§å­")
                return seed_map
            
            # ä¸ºSæ‰¾æœ€ä½³è´¨é‡ç§å­
            logger.info("ğŸ” åˆ†æSè¯´è¯äººçš„å€™é€‰è½®æ¬¡...")
            s_best_segments = self._find_best_quality_seed(s_turns, ai_segments, 'S')
            if s_best_segments:
                seed_map['S'] = s_best_segments
            
            # ä¸ºLæ‰¾æœ€ä½³è´¨é‡ç§å­ (æ’é™¤å·²ç”¨äºSçš„ç‰‡æ®µ)
            logger.info("ğŸ” åˆ†æLè¯´è¯äººçš„å€™é€‰è½®æ¬¡...")
            remaining_segments = [seg for seg in ai_segments if seg not in s_best_segments]
            l_best_segments = self._find_best_quality_seed(l_turns, remaining_segments, 'L')
            if l_best_segments:
                seed_map['L'] = l_best_segments
            
            logger.info(f"ğŸŒ± æœ€ç»ˆç§å­é€‰æ‹©ç»“æœ: S={len(seed_map['S'])}ä¸ªç‰‡æ®µ, L={len(seed_map['L'])}ä¸ªç‰‡æ®µ")
            return seed_map
            
        except Exception as e:
            logger.error(f"å¯»æ‰¾ç§å­ç‰‡æ®µå¤±è´¥: {e}")
            return {'S': [], 'L': []}
    
    def _find_best_quality_seed(self, speaker_turns, available_segments, speaker_name):
        """
        ä½¿ç”¨è´¨é‡ä¼˜å…ˆç­–ç•¥ä¸ºæŒ‡å®šè¯´è¯äººæ‰¾åˆ°æœ€ä½³ç§å­ç‰‡æ®µ
        
        Args:
            speaker_turns: è¯¥è¯´è¯äººçš„é»„é‡‘æ–‡æœ¬è½®æ¬¡
            available_segments: å¯ç”¨çš„AIç‰‡æ®µ
            speaker_name: è¯´è¯äººåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            list: æœ€ä½³è´¨é‡çš„ç§å­ç‰‡æ®µåˆ—è¡¨
        """
        # å®šä¹‰å€™é€‰èŒƒå›´ï¼šå‰5ä¸ªè½®æ¬¡
        MAX_CANDIDATES = min(5, len(speaker_turns))
        candidate_turns = speaker_turns.head(MAX_CANDIDATES)
        
        logger.info(f"   {speaker_name}è¯´è¯äººæœ‰{len(speaker_turns)}ä¸ªè½®æ¬¡ï¼Œåˆ†æå‰{MAX_CANDIDATES}ä¸ªå€™é€‰")
        
        best_quality_score = -1
        best_segments = []
        best_turn_text = ""
        
        # éå†æ¯ä¸ªå€™é€‰è½®æ¬¡
        for idx, (_, turn) in enumerate(candidate_turns.iterrows()):
            turn_text = self._clean_text_for_comparison(turn['text'])
            turn_length = len(turn_text)
            
            # ä½¿ç”¨è´ªå¿ƒå¯¹é½æ‰¾åˆ°å¯¹åº”çš„AIç‰‡æ®µ
            matched_segments = self._find_best_matching_segments(turn_text, available_segments)
            
            if matched_segments:
                # è®¡ç®—è´¨é‡åˆ†ï¼šå¹³å‡å•è¯ç½®ä¿¡åº¦
                quality_score = self._calculate_quality_score(matched_segments)
                
                logger.info(f"   å€™é€‰{idx+1}: æ–‡æœ¬é•¿åº¦={turn_length}, åŒ¹é…ç‰‡æ®µ={len(matched_segments)}ä¸ª, "
                           f"è´¨é‡åˆ†={quality_score:.4f}")
                logger.info(f"     æ–‡æœ¬: '{turn_text[:50]}...'")
                
                # é€‰æ‹©è´¨é‡åˆ†æœ€é«˜çš„
                if quality_score > best_quality_score:
                    best_quality_score = quality_score
                    best_segments = matched_segments
                    best_turn_text = turn_text
                    best_candidate_idx = idx + 1
            else:
                logger.info(f"   å€™é€‰{idx+1}: æ–‡æœ¬é•¿åº¦={turn_length}, åŒ¹é…ç‰‡æ®µ=0ä¸ª, è´¨é‡åˆ†=0.0000")
                logger.info(f"     æ–‡æœ¬: '{turn_text[:50]}...'")
        
        if best_segments:
            logger.info(f"âœ… {speaker_name}æœ€ä½³ç§å­é€‰æ‹©: å€™é€‰{best_candidate_idx}, "
                       f"è´¨é‡åˆ†={best_quality_score:.4f}, ç‰‡æ®µæ•°={len(best_segments)}")
            logger.info(f"   æœ€ä½³ç§å­æ–‡æœ¬: '{best_turn_text[:50]}...'")
            for i, seg in enumerate(best_segments[:3]):  # æ˜¾ç¤ºå‰3ä¸ªç‰‡æ®µ
                logger.info(f"   ç§å­ç‰‡æ®µ{i+1}: '{seg.get('text', '')[:30]}...'")
        else:
            logger.warning(f"âŒ {speaker_name}æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç§å­ç‰‡æ®µ")
        
        return best_segments
    
    def _calculate_quality_score(self, segments):
        """
        è®¡ç®—AIç‰‡æ®µç»„åˆçš„å¹³å‡å•è¯ç½®ä¿¡åº¦è´¨é‡åˆ†
        
        Args:
            segments: AIç‰‡æ®µåˆ—è¡¨
            
        Returns:
            float: å¹³å‡å•è¯ç½®ä¿¡åº¦ (0-1ä¹‹é—´)
        """
        total_score = 0.0
        total_words = 0
        
        for segment in segments:
            words = segment.get('words', [])
            for word in words:
                # å°è¯•å¤šç§å¯èƒ½çš„ç½®ä¿¡åº¦å­—æ®µå
                score = (word.get('score') or 
                        word.get('probability') or 
                        word.get('confidence') or 
                        0.0)
                total_score += score
                total_words += 1
        
        if total_words == 0:
            # å¦‚æœæ²¡æœ‰å•è¯çº§ä¿¡æ¯ï¼Œä½¿ç”¨ç‰‡æ®µçº§ç½®ä¿¡åº¦
            segment_scores = []
            for segment in segments:
                seg_score = (segment.get('avg_logprob') or 
                           segment.get('confidence') or 
                           0.0)
                if seg_score < 0:  # logprobè½¬æ¢ä¸ºæ¦‚ç‡
                    seg_score = max(0.0, min(1.0, (seg_score + 1.0)))
                segment_scores.append(seg_score)
            
            return sum(segment_scores) / len(segment_scores) if segment_scores else 0.0
        
        return total_score / total_words
    
    def _get_embedding_with_fresh_model(self, audio_data, segments_to_embed):
        """
        ä½¿ç”¨"ç”¨æ—¶åŠ è½½ï¼Œç”¨å®Œå³æ¯"ç­–ç•¥ç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            audio_data: Numpyæ•°ç»„æ ¼å¼çš„å®Œæ•´éŸ³é¢‘
            segments_to_embed: åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‰‡æ®µå­—å…¸çš„åˆ—è¡¨
            
        Returns:
            numpy.ndarray: å¹³å‡åµŒå…¥å‘é‡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        embedding_model = None
        
        try:
            # åŠ¨æ€åŠ è½½ç°ä»£åµŒå…¥æ¨¡å‹
            logger.info("ğŸ”„ åŠ¨æ€åŠ è½½SpeechBrainåµŒå…¥æ¨¡å‹...")
            
            # ä» speechbrain åŠ è½½ä¸€ä¸ªå¼ºå¤§çš„ã€å…¼å®¹æ€§å¥½çš„è¯´è¯äººåµŒå…¥æ¨¡å‹
            from speechbrain.inference.speaker import EncoderClassifier
            
            embedding_model = EncoderClassifier.from_hparams(
                source="speechbrain/spkrec-ecapa-voxceleb",
                savedir=os.path.join('/workspace/cache', 'speechbrain_models'),
                run_opts={"device": self.device}  # ç›´æ¥åœ¨åŠ è½½æ—¶æŒ‡å®šè®¾å¤‡
            )
            
            logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ¨æ€åŠ è½½å®Œæˆï¼Œå¤„ç†{len(segments_to_embed)}ä¸ªç‰‡æ®µ")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = []
            sample_rate = 16000
            
            for i, segment in enumerate(segments_to_embed):
                try:
                    # æå–éŸ³é¢‘ç‰‡æ®µ
                    start_sample = int(segment.get('start', 0) * sample_rate)
                    end_sample = int(segment.get('end', 0) * sample_rate)
                    
                    # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    start_sample = max(0, start_sample)
                    end_sample = min(len(audio_data), end_sample)
                    
                    if start_sample >= end_sample:
                        continue
                    
                    audio_segment = audio_data[start_sample:end_sample]
                    
                    # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ (è‡³å°‘0.1ç§’)
                    min_length = int(0.1 * sample_rate)
                    if len(audio_segment) < min_length:
                        audio_segment = np.pad(audio_segment, (0, min_length - len(audio_segment)))
                    
                    # è½¬æ¢ä¸ºPyTorch tensorå¹¶ç«‹å³å‘é€åˆ°æ­£ç¡®çš„è®¾å¤‡
                    audio_tensor = torch.from_numpy(audio_segment).float().unsqueeze(0).to(self.device)
                    
                    # ç”ŸæˆåµŒå…¥ - speechbrain æ¨¡å‹ç›´æ¥æ¥æ”¶éŸ³é¢‘å¼ é‡å’Œå…¶ç›¸å¯¹é•¿åº¦
                    with torch.no_grad():
                        wav_lens = torch.tensor([1.0], device=self.device)  # 1.0 è¡¨ç¤ºä½¿ç”¨å®Œæ•´é•¿åº¦
                        embedding = embedding_model.encode_batch(audio_tensor, wav_lens=wav_lens)
                    
                    # è½¬æ¢ä¸ºnumpy - ç§»é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦ï¼Œç„¶åå±•å¹³
                    if isinstance(embedding, torch.Tensor):
                        embedding = embedding.squeeze().cpu().numpy()
                    
                    if embedding is not None:
                        embeddings.append(embedding)
                        
                except Exception as e:
                    logger.warning(f"ç‰‡æ®µ{i}åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    continue
            
            if embeddings:
                # è®¡ç®—å¹³å‡åµŒå…¥
                mean_embedding = np.mean(embeddings, axis=0)
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆå¹³å‡åµŒå…¥å‘é‡ï¼Œå½¢çŠ¶: {mean_embedding.shape}")
                return mean_embedding
            else:
                logger.warning("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•åµŒå…¥å‘é‡")
                return None
                
        except Exception as e:
            logger.error(f"åŠ¨æ€åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            return None
            
        finally:
            # æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½è¦æ¸…ç†æ¨¡å‹
            if embedding_model is not None:
                logger.info("ğŸ—‘ï¸ æ¸…ç†åµŒå…¥æ¨¡å‹...")
                del embedding_model
                
                # æ¸…ç†CUDAç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                logger.info("âœ… åµŒå…¥æ¨¡å‹å·²å¸è½½å’Œæ¸…ç†")
    
    def _generate_single_embedding_with_model(self, audio_segment, embedding_model):
        """
        ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ä¸ºå•ä¸ªéŸ³é¢‘ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            audio_segment: éŸ³é¢‘ç‰‡æ®µ (numpy array)
            embedding_model: å·²åŠ è½½çš„åµŒå…¥æ¨¡å‹
            
        Returns:
            numpy.ndarray: åµŒå…¥å‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ (è‡³å°‘0.1ç§’)
            min_length = int(0.1 * 16000)
            if len(audio_segment) < min_length:
                # å¦‚æœå¤ªçŸ­ï¼Œç”¨é›¶å¡«å……
                audio_segment = np.pad(audio_segment, (0, min_length - len(audio_segment)))
            
            # è½¬æ¢ä¸ºPyTorch tensorå¹¶ç«‹å³å‘é€åˆ°æ­£ç¡®çš„è®¾å¤‡
            audio_tensor = torch.from_numpy(audio_segment).float().unsqueeze(0).to(self.device)
            
            # ç”ŸæˆåµŒå…¥ - speechbrain æ¨¡å‹è°ƒç”¨æ–¹å¼
            with torch.no_grad():
                wav_lens = torch.tensor([1.0], device=self.device)  # 1.0 è¡¨ç¤ºä½¿ç”¨å®Œæ•´é•¿åº¦
                embedding = embedding_model.encode_batch(audio_tensor, wav_lens=wav_lens)
            
            # è½¬æ¢ä¸ºnumpy - ç§»é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦ï¼Œç„¶åå±•å¹³
            if isinstance(embedding, torch.Tensor):
                embedding = embedding.squeeze().cpu().numpy()
            
            return embedding
            
        except Exception as e:
            logger.warning(f"å•ä¸ªåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _find_best_matching_segments(self, target_text, ai_segments):
        """
        ä½¿ç”¨è´ªå¿ƒç®—æ³•æ‰¾åˆ°ä¸ç›®æ ‡æ–‡æœ¬æœ€åŒ¹é…çš„AIç‰‡æ®µç»„åˆ
        
        Args:
            target_text: æ¸…ç†åçš„ç›®æ ‡æ–‡æœ¬
            ai_segments: å¯ç”¨çš„AIç‰‡æ®µåˆ—è¡¨
            
        Returns:
            list: æœ€ä½³åŒ¹é…çš„ç‰‡æ®µåˆ—è¡¨
        """
        if not target_text or not ai_segments:
            return []
        
        best_match_ratio = 0
        best_match_segments = []
        
        # è´ªå¿ƒæœç´¢ï¼šå°è¯•ä¸åŒçš„ç‰‡æ®µç»„åˆ
        for start_idx in range(len(ai_segments)):
            temp_text = ""
            temp_segments = []
            
            for end_idx in range(start_idx, min(start_idx + 5, len(ai_segments))):  # æœ€å¤šç»„åˆ5ä¸ªç‰‡æ®µ
                segment = ai_segments[end_idx]
                temp_segments.append(segment)
                temp_text += " " + self._clean_text_for_comparison(segment.get('text', ''))
                temp_text = temp_text.strip()
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                if temp_text:
                    similarity = difflib.SequenceMatcher(None, temp_text, target_text).ratio()
                    
                    if similarity > best_match_ratio:
                        best_match_ratio = similarity
                        best_match_segments = temp_segments.copy()
                    
                    # å¦‚æœç›¸ä¼¼åº¦å¼€å§‹ä¸‹é™ï¼Œæå‰åœæ­¢
                    if len(temp_segments) > 1 and similarity < best_match_ratio * 0.8:
                        break
        
        logger.info(f"æœ€ä½³åŒ¹é…ç›¸ä¼¼åº¦: {best_match_ratio:.3f}")
        return best_match_segments
    
    def perform_seed_based_diarization(self, audio_data, all_ai_segments, seed_map):
        """
        åŸºäºç§å­ç‰‡æ®µè¿›è¡Œè¯´è¯äººè¯†åˆ«çš„æ ¸å¿ƒå‡½æ•°
        
        Args:
            audio_data: whisperx.load_audio()è¿”å›çš„numpyæ•°ç»„
            all_ai_segments: å½“å‰å¯¹è¯çš„æ‰€æœ‰AIè½¬å½•ç‰‡æ®µ
            seed_map: ç§å­å­—å…¸ {'S': [...], 'L': [...]}
            
        Returns:
            tuple: (æ›´æ–°äº†speakerå­—æ®µçš„all_ai_segments, æˆåŠŸæ ‡å¿—boolean)
        """
        logger.info("å¼€å§‹åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«...")
        
        try:
            # 1. ä½¿ç”¨åŠ¨æ€åŠ è½½ç­–ç•¥ç”Ÿæˆç§å­æŒ‡çº¹
            logger.info("æ­¥éª¤1: ç”ŸæˆSè¯´è¯äººçš„ç§å­æŒ‡çº¹...")
            s_seed_embedding = self._get_embedding_with_fresh_model(audio_data, seed_map['S'])
            
            logger.info("æ­¥éª¤2: ç”ŸæˆLè¯´è¯äººçš„ç§å­æŒ‡çº¹...")
            l_seed_embedding = self._get_embedding_with_fresh_model(audio_data, seed_map['L'])
            
            if s_seed_embedding is None or l_seed_embedding is None:
                logger.error("æ— æ³•ç”Ÿæˆä¸€ä¸ªæˆ–ä¸¤ä¸ªç§å­åµŒå…¥ï¼Œè·³è¿‡è¯´è¯äººè¯†åˆ«")
                return all_ai_segments, False
            
            # --- ç§å­è‡ªæ£€é€»è¾‘ ---
            seeds_similarity = cosine_similarity(
                s_seed_embedding.reshape(1, -1),
                l_seed_embedding.reshape(1, -1)
            )[0][0]
            
            logger.info(f"ğŸ” ç§å­è‡ªæ£€ï¼šSå’ŒLçš„ç§å­æŒ‡çº¹ç›¸ä¼¼åº¦ä¸º {seeds_similarity:.4f}")
            
            # å¦‚æœä¸¤ä¸ªç§å­è¿‡äºç›¸ä¼¼ï¼Œåˆ™æ²¡æœ‰ç»§ç»­ä¸‹å»çš„æ„ä¹‰
            SIMILARITY_THRESHOLD = 0.85  # è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„é˜ˆå€¼
            if seeds_similarity > SIMILARITY_THRESHOLD:
                logger.error(f"âŒ ç§å­è¿‡äºç›¸ä¼¼ (ç›¸ä¼¼åº¦>{SIMILARITY_THRESHOLD})ï¼Œæ— æ³•åŒºåˆ†è¯´è¯äººã€‚è¯·æ£€æŸ¥ç§å­é€‰æ‹©é€»è¾‘æˆ–éŸ³é¢‘è´¨é‡ã€‚")
                # ç›´æ¥è¿”å›ï¼Œæ ‡è®°æ‰€æœ‰ä¸ºUNKNOWNå¹¶è®¾ç½®å¤±è´¥
                for segment in all_ai_segments:
                    segment['speaker'] = 'UNKNOWN'
                    segment['confidence'] = 0.0
                return all_ai_segments, False
            # --- è‡ªæ£€é€»è¾‘ç»“æŸ ---
            
            logger.info("âœ… ç§å­åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œç§å­å·®å¼‚å……è¶³")
            
            # 3. ä¸ºä¸»è¦è¯†åˆ«æµç¨‹é¢„åŠ è½½ä¸€ä¸ª"å¹²å‡€"çš„æ¨¡å‹å®ä¾‹
            logger.info("æ­¥éª¤3: ä¸ºä¸»è¦è¯†åˆ«æµç¨‹åŠ¨æ€åŠ è½½æ¨¡å‹...")
            main_embedding_model = None
            
            try:
                # ä½¿ç”¨ speechbrain ç°ä»£åµŒå…¥æ¨¡å‹
                from speechbrain.inference.speaker import EncoderClassifier
                
                main_embedding_model = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir=os.path.join('/workspace/cache', 'speechbrain_models'),
                    run_opts={"device": self.device}  # ç›´æ¥åœ¨åŠ è½½æ—¶æŒ‡å®šè®¾å¤‡
                )
                
                logger.info("âœ… ä¸»è¦è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
                
                # 4. è¯†åˆ«æ‰€æœ‰ç‰‡æ®µ
                sample_rate = 16000  # WhisperXä½¿ç”¨16kHz
                
                for i, segment in enumerate(all_ai_segments):
                    try:
                        # æå–éŸ³é¢‘ç‰‡æ®µ
                        start_sample = int(segment.get('start', 0) * sample_rate)
                        end_sample = int(segment.get('end', 0) * sample_rate)
                        
                        # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                        start_sample = max(0, start_sample)
                        end_sample = min(len(audio_data), end_sample)
                        
                        if start_sample >= end_sample:
                            logger.warning(f"ç‰‡æ®µ{i}æ—¶é—´æˆ³æ— æ•ˆï¼Œè·³è¿‡")
                            segment['speaker'] = 'UNKNOWN'
                            continue
                        
                        audio_segment = audio_data[start_sample:end_sample]
                        
                        # ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹ç”Ÿæˆç‰‡æ®µåµŒå…¥
                        segment_embedding = self._generate_single_embedding_with_model(audio_segment, main_embedding_model)
                        
                        if segment_embedding is not None:
                            # è®¡ç®—ä¸ç§å­çš„ç›¸ä¼¼åº¦
                            s_similarity = cosine_similarity(
                                segment_embedding.reshape(1, -1), 
                                s_seed_embedding.reshape(1, -1)
                            )[0][0]
                            
                            l_similarity = cosine_similarity(
                                segment_embedding.reshape(1, -1), 
                                l_seed_embedding.reshape(1, -1)
                            )[0][0]
                            
                            # é‡æ„çš„è¯´è¯äººåˆ†é…é€»è¾‘ - åˆ†æ­¥å†³ç­–æµç¨‹
                            
                            # 1. å®šä¹‰é˜ˆå€¼
                            MIN_CONFIDENCE_THRESHOLD = 0.45  # ç¨å¾®é™ä½æœ€å°ç½®ä¿¡åº¦
                            MIN_DIFFERENCE_THRESHOLD = 0.08  # ç¨å¾®é™ä½å·®è·è¦æ±‚
                            
                            confidence = 0.0
                            assigned_speaker = 'UNKNOWN'
                            
                            # 3. ç¡®å®šèƒœå‡ºæ–¹
                            if s_similarity > l_similarity:
                                winner = 'S'
                                winner_score = s_similarity
                                loser_score = l_similarity
                            else:
                                winner = 'L'
                                winner_score = l_similarity
                                loser_score = s_similarity
                            
                            # 4. åˆ†æ­¥éªŒè¯ç»“æœæ˜¯å¦å¯ä¿¡
                            # æ¡ä»¶1: èƒœå‡ºæ–¹çš„åˆ†æ•°æ˜¯å¦è¾¾åˆ°äº†æœ€ä½è¦æ±‚ï¼Ÿ
                            is_confident_enough = winner_score >= MIN_CONFIDENCE_THRESHOLD
                            
                            # æ¡ä»¶2: èƒœå‡ºæ–¹å’Œå¤±è´¥æ–¹çš„åˆ†æ•°å·®è·æ˜¯å¦è¶³å¤Ÿå¤§ï¼Ÿ
                            is_distinct_enough = (winner_score - loser_score) >= MIN_DIFFERENCE_THRESHOLD
                            
                            # åªæœ‰å½“ä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³æ—¶ï¼Œæˆ‘ä»¬æ‰æ¥å—è¿™ä¸ªç»“æœ
                            if is_confident_enough and is_distinct_enough:
                                assigned_speaker = winner
                                confidence = winner_score
                            else:
                                # å¦åˆ™ï¼Œå³ä½¿æœ‰ä¸€æ–¹åˆ†æ•°æ›´é«˜ï¼Œæˆ‘ä»¬ä¾ç„¶è®¤ä¸ºç»“æœä¸å¯é 
                                assigned_speaker = 'UNKNOWN'
                                confidence = winner_score  # ä¾ç„¶å¯ä»¥è®°å½•æœ€é«˜åˆ†ï¼Œä½†æ ‡ç­¾æ˜¯UNKNOWN
                            
                            segment['speaker'] = assigned_speaker
                            segment['confidence'] = float(confidence)
                            
                            # è°ƒè¯•æ—¥å¿— - æ˜¾ç¤ºå‰å‡ ä¸ªç‰‡æ®µçš„è¯¦ç»†ä¿¡æ¯
                            if i < 5:
                                if assigned_speaker != 'UNKNOWN':
                                    logger.info(f"âœ… ç‰‡æ®µ {i}: S={s_similarity:.3f}, L={l_similarity:.3f}, "
                                               f"èƒœå‡º={winner}({winner_score:.3f}), å·®è·={winner_score-loser_score:.3f}, "
                                               f"åˆ†é…={assigned_speaker}")
                                else:
                                    reason = []
                                    if not is_confident_enough:
                                        reason.append(f"ç½®ä¿¡åº¦ä¸è¶³({winner_score:.3f}<{MIN_CONFIDENCE_THRESHOLD})")
                                    if not is_distinct_enough:
                                        reason.append(f"å·®è·ä¸å¤Ÿ({winner_score-loser_score:.3f}<{MIN_DIFFERENCE_THRESHOLD})")
                                    
                                    logger.warning(f"âŒ ç‰‡æ®µ {i}: S={s_similarity:.3f}, L={l_similarity:.3f}, "
                                                 f"èƒœå‡º={winner}({winner_score:.3f}), æ ‡è®°=UNKNOWN, "
                                                 f"åŸå› : {', '.join(reason)}")
                            
                        else:
                            logger.warning(f"ç‰‡æ®µ{i}æ— æ³•ç”ŸæˆåµŒå…¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡è®°")
                            segment['speaker'] = 'UNKNOWN'
                            segment['confidence'] = None
                        
                    except Exception as e:
                        logger.warning(f"å¤„ç†ç‰‡æ®µ{i}å¤±è´¥: {e}")
                        segment['speaker'] = 'UNKNOWN'
                        segment['confidence'] = None
                
                # ç»Ÿè®¡ç»“æœå’Œç›¸ä¼¼åº¦åˆ†å¸ƒ
                s_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'S')
                l_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'L')
                unknown_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'UNKNOWN')
                
                # ç»Ÿè®¡ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆç”¨äºè°ƒè¯•é—¨æ§›è®¾ç½®ï¼‰
                confidences = [seg.get('confidence', 0) for seg in all_ai_segments if seg.get('confidence') is not None]
                if confidences:
                    avg_confidence = sum(confidences) / len(confidences)
                    max_confidence = max(confidences)
                    min_confidence = min(confidences)
                    logger.info(f"ç½®ä¿¡åº¦ç»Ÿè®¡: å¹³å‡={avg_confidence:.3f}, æœ€é«˜={max_confidence:.3f}, æœ€ä½={min_confidence:.3f}")
                
                logger.info(f"è¯´è¯äººè¯†åˆ«ç»“æœ: S={s_count}, L={l_count}, Unknown={unknown_count}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æˆåŠŸè¯†åˆ«
                success_rate = (s_count + l_count) / len(all_ai_segments) if all_ai_segments else 0
                success = success_rate > 0.5  # è¶…è¿‡50%æˆåŠŸæ‰ç®—æˆåŠŸ
                
                logger.info(f"è¯†åˆ«æˆåŠŸç‡: {success_rate:.2%}, æ•´ä½“çŠ¶æ€: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
                return all_ai_segments, success
                
            finally:
                # ç¡®ä¿ä¸»è¦è¯†åˆ«æ¨¡å‹è¢«æ¸…ç†
                if main_embedding_model is not None:
                    logger.info("ğŸ—‘ï¸ æ¸…ç†ä¸»è¦è¯†åˆ«æ¨¡å‹...")
                    del main_embedding_model
                    
                    # æ¸…ç†CUDAç¼“å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    logger.info("âœ… ä¸»è¦è¯†åˆ«æ¨¡å‹å·²å¸è½½å’Œæ¸…ç†")
                
        except Exception as e:
            logger.error(f"åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«å¤±è´¥: {e}")
            return all_ai_segments, False
    
    def _generate_seed_embedding(self, audio_data, seed_segments):
        """
        ä¸ºç§å­ç‰‡æ®µç”Ÿæˆå¹³å‡åµŒå…¥å‘é‡
        
        Args:
            audio_data: å®Œæ•´éŸ³é¢‘æ•°æ®
            seed_segments: ç§å­ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            numpy.ndarray: å¹³å‡åµŒå…¥å‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        if not seed_segments:
            return None
        
        sample_rate = 16000
        embeddings = []
        
        for segment in seed_segments:
            try:
                start_sample = int(segment.get('start', 0) * sample_rate)
                end_sample = int(segment.get('end', 0) * sample_rate)
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                start_sample = max(0, start_sample)
                end_sample = min(len(audio_data), end_sample)
                
                if start_sample >= end_sample:
                    continue
                
                audio_segment = audio_data[start_sample:end_sample]
                embedding = self._generate_single_embedding(audio_segment)
                
                if embedding is not None:
                    embeddings.append(embedding)
                    
            except Exception as e:
                logger.warning(f"ç§å­ç‰‡æ®µåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                continue
        
        if embeddings:
            # è®¡ç®—å¹³å‡åµŒå…¥
            mean_embedding = np.mean(embeddings, axis=0)
            return mean_embedding
        else:
            return None
    
    def _generate_single_embedding(self, audio_segment):
        """
        ä¸ºå•ä¸ªéŸ³é¢‘ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            audio_segment: éŸ³é¢‘ç‰‡æ®µ (numpy array)
            
        Returns:
            numpy.ndarray: åµŒå…¥å‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ (è‡³å°‘0.1ç§’)
            min_length = int(0.1 * 16000)
            if len(audio_segment) < min_length:
                # å¦‚æœå¤ªçŸ­ï¼Œç”¨é›¶å¡«å……
                audio_segment = np.pad(audio_segment, (0, min_length - len(audio_segment)))
            
            # è½¬æ¢ä¸ºPyTorch tensor
            audio_tensor = torch.from_numpy(audio_segment).float().unsqueeze(0)
            
            if self.device == "cuda":
                audio_tensor = audio_tensor.cuda()
            
            # ç”ŸæˆåµŒå…¥ - ä½¿ç”¨Modelçš„æ­£ç¡®è°ƒç”¨æ–¹å¼
            with torch.no_grad():
                # ç›´æ¥ä¼ é€’tensorç»™æ¨¡å‹ï¼Œä¸ä½¿ç”¨dictæ ¼å¼
                # pyannote ModelæœŸæœ›ç›´æ¥æ¥æ”¶waveform tensor
                embedding = self.embedding_model(audio_tensor)
            
            # è½¬æ¢ä¸ºnumpy
            if isinstance(embedding, torch.Tensor):
                embedding = embedding.cpu().numpy()
            
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆå•ä¸ªåµŒå…¥å¤±è´¥: {e}")
            return None
    
    def process_single_file(self, audio_path, dyad_id, conversation_id, golden_turns_df=None):
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¤„ç†: {Path(audio_path).name}")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½éŸ³é¢‘
            logger.info("åŠ è½½éŸ³é¢‘...")
            audio = whisperx.load_audio(audio_path)
            duration = len(audio) / 16000
            logger.info(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
            
            # 2. è½¬å½•ï¼ˆLarge-v3é«˜ç²¾åº¦ï¼‰
            logger.info("å¼€å§‹é«˜ç²¾åº¦è½¬å½•...")
            result = self.model.transcribe(
                audio,
                batch_size=self.config['batch_size']
            )
            
            segments = result.get("segments", [])
            logger.info(f"è½¬å½•å®Œæˆ: {len(segments)}ä¸ªç‰‡æ®µ")
            
            # 3. å¼ºåˆ¶å¯¹é½ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            if self.align_model and segments:
                logger.info("è¿›è¡Œå¼ºåˆ¶å¯¹é½...")
                result = whisperx.align(
                    result["segments"], 
                    self.align_model, 
                    self.metadata, 
                    audio, 
                    self.device, 
                    return_char_alignments=False
                )
                logger.info("âœ… å¼ºåˆ¶å¯¹é½å®Œæˆ")
            
            # 4. æ–°æ··åˆç­–ç•¥ä¸‰é˜¶æ®µè¯´è¯äººè¯†åˆ«æµç¨‹
            speaker_success = False
            if golden_turns_df is not None and not golden_turns_df.empty:
                
                # --- é˜¶æ®µä¸€ï¼šå¼ºåˆ¶åˆ†é…å¼€åœºï¼ˆå‰3è½®ï¼‰ ---
                logger.info(">> é˜¶æ®µä¸€ï¼šå¼ºåˆ¶åˆ†é…å‰3è½®è¯´è¯äºº...")
                # è°ƒç”¨ä¸€ä¸ªæ–°å‡½æ•°æ¥å¤„ç†è¿™ä¸ªé€»è¾‘ï¼Œå®ƒä¼šè¿”å›å·²è¢«åˆ†é…å¥½è¯´è¯äººçš„ç‰‡æ®µï¼Œä»¥åŠå‰©ä½™æœªåˆ†é…çš„ç‰‡æ®µ
                all_segments = result["segments"]
                processed_segments, remaining_segments, success_stage1 = self._force_align_initial_turns(
                    golden_turns_df, 
                    all_segments,
                    num_turns=3  # æŒ‡å®šå¼ºåˆ¶åˆ†é…çš„è½®æ¬¡æ•°
                )

                # --- é˜¶æ®µäºŒ å’Œ é˜¶æ®µä¸‰ ---
                if success_stage1 and remaining_segments:
                    logger.info(">> é˜¶æ®µäºŒï¼šä»åç»­è½®æ¬¡ä¸­æ™ºèƒ½é€‰æ‹©ç§å­...")

                    # ç§å­é€‰æ‹©èŒƒå›´ä»ç¬¬4è½®å¼€å§‹ (å› ä¸ºå‰3è½®å·²ç”¨æ‰)
                    seed_candidate_turns = golden_turns_df.iloc[3:]

                    # è°ƒç”¨ _find_seed_segmentsï¼Œä½†åªåœ¨å€™é€‰è½®æ¬¡å’Œå‰©ä½™ç‰‡æ®µä¸­å¯»æ‰¾
                    seed_map = self._find_seed_segments(seed_candidate_turns, remaining_segments)

                    if seed_map.get('S') and seed_map.get('L'):
                        logger.info(">> é˜¶æ®µä¸‰ï¼šå¯¹å‰©ä½™ç‰‡æ®µè¿›è¡Œç§å­è¯†åˆ«...")

                        # è°ƒç”¨ perform_seed_based_diarizationï¼Œä½†åªå¤„ç†å‰©ä½™çš„ç‰‡æ®µ
                        diarized_remaining_segments, success_stage3 = self.perform_seed_based_diarization(
                            audio,
                            remaining_segments,
                            seed_map
                        )

                        # åˆå¹¶ç»“æœ
                        final_segments = processed_segments + diarized_remaining_segments
                        speaker_success = True
                    else:
                        logger.warning("æœªèƒ½ä»åç»­è½®æ¬¡ä¸­æ‰¾åˆ°è¶³å¤Ÿçš„ç§å­ï¼Œå‰©ä½™ç‰‡æ®µå°†ä½¿ç”¨å›é€€æ–¹æ¡ˆã€‚")
                        # å¯¹å‰©ä½™éƒ¨åˆ†ä½¿ç”¨å›é€€æ–¹æ¡ˆ
                        for i, seg in enumerate(remaining_segments):
                            seg['speaker'] = 'UNKNOWN'  # æˆ– A/B è½®æ¢
                        final_segments = processed_segments + remaining_segments
                        speaker_success = False  # æ•´ä½“ä¸ç®—å®Œå…¨æˆåŠŸ
                else:
                    logger.warning("é˜¶æ®µä¸€å¤±è´¥æˆ–æ²¡æœ‰å‰©ä½™ç‰‡æ®µï¼Œç›´æ¥ä½¿ç”¨é˜¶æ®µä¸€çš„ç»“æœã€‚")
                    final_segments = processed_segments
                    speaker_success = success_stage1

                result["segments"] = final_segments

            else:
                logger.warning("æœªæä¾›é»„é‡‘æ–‡æœ¬ï¼Œè·³è¿‡è¯´è¯äººè¯†åˆ«")
                speaker_success = False
            
            # 5. å¤„ç†å’Œæ ¼å¼åŒ–ç»“æœ
            processed_segments = self._format_results(
                result.get("segments", []), 
                dyad_id, 
                conversation_id, 
                speaker_success,
                duration
            )
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {processing_time:.1f}ç§’")
            
            # æ¸…ç†GPUå†…å­˜
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            
            return processed_segments
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _format_results(self, segments, dyad_id, conversation_id, speaker_success, total_duration):
        """æ ¼å¼åŒ–è½¬å½•ç»“æœ"""
        processed = []
        
        # è°ƒè¯•ï¼šè®°å½•ç¬¬ä¸€ä¸ªsegmentçš„ç»“æ„
        if segments and len(segments) > 0:
            logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªsegmentåŒ…å«çš„å­—æ®µ: {list(segments[0].keys())}")
        
        for i, seg in enumerate(segments):
            # å¤„ç†è¯´è¯äººä¿¡æ¯
            if speaker_success and "speaker" in seg:
                speaker_raw = seg["speaker"]
                # å¤„ç†æ–°çš„S/Læ ‡è¯†ç³»ç»Ÿ
                if speaker_raw == 'S':
                    speaker_name = "Speaker_A"
                    speaker_raw = "SPEAKER_00"
                elif speaker_raw == 'L':
                    speaker_name = "Speaker_B"
                    speaker_raw = "SPEAKER_01"
                elif "SPEAKER_00" in str(speaker_raw):
                    speaker_name = "Speaker_A"
                elif "SPEAKER_01" in str(speaker_raw):
                    speaker_name = "Speaker_B"
                else:
                    # å¤„ç†å…¶ä»–æƒ…å†µ
                    speaker_name = f"Speaker_{str(speaker_raw).split('_')[-1]}"
            else:
                # ç®€å•äº¤æ›¿åˆ†é…
                speaker_name = f"Speaker_{'A' if i % 2 == 0 else 'B'}"
                speaker_raw = f"SPEAKER_{i % 2:02d}"
            
            # è·å–æ—¶é—´æˆ³
            start_seconds = seg.get('start', 0)
            end_seconds = seg.get('end', 0)
            duration = end_seconds - start_seconds
            
            # è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
            start_time = self._seconds_to_timestamp(start_seconds)
            finish_time = self._seconds_to_timestamp(end_seconds)
            
            # è·å–æ–‡æœ¬å’Œç½®ä¿¡åº¦
            text = seg.get('text', '').strip()
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä»å¤šä¸ªå¯èƒ½çš„æºï¼‰
            confidence = 0.0
            if 'confidence' in seg and seg['confidence'] is not None:
                confidence = seg['confidence']
            elif 'avg_logprob' in seg:
                # å°†logprobè½¬æ¢ä¸ºç½®ä¿¡åº¦è¿‘ä¼¼å€¼
                confidence = min(1.0, max(0.0, (seg['avg_logprob'] + 1.0)))
            elif 'words' in seg and seg['words']:
                # ä»è¯çº§ç½®ä¿¡åº¦è®¡ç®—å¹³å‡å€¼
                word_confidences = [w.get('probability', 0.0) for w in seg['words'] if 'probability' in w]
                if word_confidences:
                    confidence = sum(word_confidences) / len(word_confidences)
                else:
                    # å¦‚æœæ²¡æœ‰ä»»ä½•ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œæ ¹æ®æ— åœé¡¿æ—¶é—´ä¼°ç®—
                    confidence = 0.85 if seg.get('no_speech_prob', 1.0) < 0.5 else 0.3
            else:
                # é»˜è®¤åˆç†ç½®ä¿¡åº¦ï¼ˆè€Œé0ï¼‰
                confidence = 0.8
            
            # ç¡®ä¿confidenceæ˜¯æœ‰æ•ˆæ•°å€¼
            if confidence is None:
                confidence = 0.5  # è®¾ç½®åˆç†çš„é»˜è®¤å€¼
            
            # ç»Ÿè®¡è¯çº§ä¿¡æ¯
            words = seg.get('words', [])
            word_count = len(text.split()) if text else 0
            
            processed.append({
                'dyad': dyad_id,
                'conversation': conversation_id,
                'segment_id': i + 1,
                'start_time': start_time,  # æ–°æ ¼å¼ï¼šHH:MM:SS,mmm
                'finish_time': finish_time,  # æ”¹åä¸ºfinish_time
                'duration': round(duration, 3),
                'speaker': speaker_name,
                'speaker_raw': speaker_raw,
                'text': text,
                'confidence': round(confidence, 4),
                'word_count': word_count,
                'language': 'nl',  # è·å…°è¯­
                'model_used': 'large-v3',
                'device_used': self.device,
                'has_ai_speaker_detection': speaker_success
            })
        
        return processed
    
    def cleanup(self):
        """æ¸…ç†æ¨¡å‹å’Œå†…å­˜"""
        if self.model:
            del self.model
        if self.align_model:
            del self.align_model
        if self.diarize_model:
            del self.diarize_model
        if self.embedding_model:
            del self.embedding_model
        
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("å†…å­˜æ¸…ç†å®Œæˆ")


def load_golden_text_data(golden_text_path):
    """åŠ è½½é»„é‡‘æ ‡å‡†æ–‡æœ¬æ•°æ®"""
    try:
        logger.info(f"åŠ è½½é»„é‡‘æ–‡æœ¬æ•°æ®: {golden_text_path}")
        df = pd.read_csv(golden_text_path)
        df.columns = df.columns.str.strip()  # æ¸…ç†åˆ—å
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['dyad', 'conversation', 'role', 'text']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.error(f"é»„é‡‘æ–‡æœ¬ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            logger.info(f"å¯ç”¨åˆ—: {list(df.columns)}")
            return None
        
        logger.info(f"âœ… é»„é‡‘æ–‡æœ¬åŠ è½½å®Œæˆ: {len(df)}è¡Œæ•°æ®")
        logger.info(f"åŒ…å«å¯¹è¯: {df['dyad'].nunique()}ä¸ªdyad, {df['conversation'].nunique()}ä¸ªconversation")
        
        return df
        
    except Exception as e:
        logger.error(f"åŠ è½½é»„é‡‘æ–‡æœ¬å¤±è´¥: {e}")
        return None


def process_conversations_with_golden_text(
    audio_dir, 
    golden_text_path, 
    output_dir, 
    conversation_mapping=None
):
    """
    ä½¿ç”¨é»„é‡‘æ–‡æœ¬æ•°æ®æ‰¹é‡å¤„ç†å¯¹è¯
    
    Args:
        audio_dir: éŸ³é¢‘æ–‡ä»¶ç›®å½•
        golden_text_path: é»„é‡‘æ–‡æœ¬CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        conversation_mapping: éŸ³é¢‘æ–‡ä»¶ååˆ°(dyad, conversation)çš„æ˜ å°„å­—å…¸
                            å¦‚æœä¸ºNoneï¼Œå°†å°è¯•ä»æ–‡ä»¶åè§£æ
    """
    
    # åŠ è½½é»„é‡‘æ–‡æœ¬
    golden_df = load_golden_text_data(golden_text_path)
    if golden_df is None:
        logger.error("æ— æ³•åŠ è½½é»„é‡‘æ–‡æœ¬ï¼Œç»ˆæ­¢å¤„ç†")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = HighPrecisionAudioProcessor()
    
    try:
        # åŠ è½½æ¨¡å‹
        if not processor.load_models():
            logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # è·å–éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        audio_files = []
        for ext in ['.wav', '.mp3', '.m4a', '.flac']:
            audio_files.extend(Path(audio_dir).glob(f"*{ext}"))
        
        logger.info(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        all_results = []
        processed_count = 0
        
        for audio_file in audio_files:
            try:
                # è§£ædyadå’Œconversation
                if conversation_mapping:
                    if audio_file.name in conversation_mapping:
                        dyad_id, conversation_id = conversation_mapping[audio_file.name]
                    else:
                        logger.warning(f"æ–‡ä»¶ {audio_file.name} ä¸åœ¨æ˜ å°„ä¸­ï¼Œè·³è¿‡")
                        continue
                else:
                    # å°è¯•ä»æ–‡ä»¶åè§£æï¼Œæ”¯æŒå¤šç§æ ¼å¼
                    try:
                        # æ ¼å¼1: dyad_X_conversation_Y.mp3
                        if '_' in audio_file.stem:
                            parts = audio_file.stem.split('_')
                            dyad_id = int(parts[1])
                            conversation_id = int(parts[3])
                        # æ ¼å¼2: X.Y.mp3 (dyad.conversation.mp3)
                        elif '.' in audio_file.stem:
                            parts = audio_file.stem.split('.')
                            dyad_id = int(parts[0])
                            conversation_id = int(parts[1])
                        else:
                            raise ValueError("æ— æ³•è¯†åˆ«çš„æ–‡ä»¶åæ ¼å¼")
                            
                        logger.info(f"ğŸ“ è§£ææ–‡ä»¶ {audio_file.name} -> dyad:{dyad_id}, conversation:{conversation_id}")
                    except (ValueError, IndexError):
                        logger.warning(f"æ— æ³•ä»æ–‡ä»¶åè§£ædyadå’Œconversation: {audio_file.name}")
                        logger.info("æ”¯æŒçš„æ ¼å¼: dyad_X_conversation_Y.mp3 æˆ– X.Y.mp3")
                        continue
                
                # è¿‡æ»¤å¯¹åº”çš„é»„é‡‘æ–‡æœ¬
                golden_turns_df = golden_df[
                    (golden_df['dyad'] == dyad_id) & 
                    (golden_df['conversation'] == conversation_id)
                ].copy()
                
                if golden_turns_df.empty:
                    logger.warning(f"å¯¹è¯ {dyad_id}-{conversation_id} æ²¡æœ‰å¯¹åº”çš„é»„é‡‘æ–‡æœ¬ï¼Œè·³è¿‡")
                    continue
                
                logger.info(f"å¤„ç†å¯¹è¯ {dyad_id}-{conversation_id}: {audio_file.name}")
                logger.info(f"é»„é‡‘æ–‡æœ¬è½®æ¬¡: {len(golden_turns_df)}")
                
                # å¤„ç†éŸ³é¢‘æ–‡ä»¶
                segments = processor.process_single_file(
                    str(audio_file), 
                    dyad_id, 
                    conversation_id, 
                    golden_turns_df
                )
                
                all_results.extend(segments)
                processed_count += 1
                
                logger.info(f"âœ… å®Œæˆå¤„ç† {dyad_id}-{conversation_id}: {len(segments)}ä¸ªç‰‡æ®µ")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {audio_file.name} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        if all_results:
            output_file = os.path.join(output_dir, "combined_transcription_with_seed_diarization.csv")
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(output_file, index=False, encoding='utf-8')
            
            logger.info(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"æ€»è®¡å¤„ç†: {processed_count}ä¸ªå¯¹è¯, {len(all_results)}ä¸ªç‰‡æ®µ")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            if 'has_ai_speaker_detection' in results_df.columns:
                success_count = results_df['has_ai_speaker_detection'].sum()
                logger.info(f"è¯´è¯äººè¯†åˆ«æˆåŠŸç‡: {success_count}/{processed_count} ({success_count/processed_count*100:.1f}%)")
        else:
            logger.warning("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
    
    finally:
        # æ¸…ç†
        processor.cleanup()


def main():
    """ä¸»å‡½æ•° - RunPodä½¿ç”¨ç¤ºä¾‹"""
    
    print("ğŸš€ WhisperXåŠè‡ªåŠ¨ç§å­è¯†åˆ«è¯´è¯äººæ—¥å¿—ç³»ç»Ÿ")
    print("=" * 50)
    
    # è·³è¿‡è€—æ—¶çš„ç¯å¢ƒæ£€æŸ¥ï¼Œç›´æ¥å¼€å§‹
    print("âš¡ å¼€å§‹å¤„ç†...")
    
    # é»˜è®¤RunPodè·¯å¾„é…ç½®
    audio_directory = "/workspace/input"
    golden_text_file = "/workspace/input/text_data_output.csv"
    output_directory = "/workspace/output"
    
    print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶ç›®å½•: {audio_directory}")
    print(f"ğŸ“„ é»„é‡‘æ–‡æœ¬æ–‡ä»¶: {golden_text_file}")
    print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {output_directory}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶å’Œç›®å½•
    if not os.path.exists(audio_directory):
        print(f"âŒ éŸ³é¢‘ç›®å½•ä¸å­˜åœ¨: {audio_directory}")
        print("è¯·å°†éŸ³é¢‘æ–‡ä»¶(.wav æˆ– .mp3)æ”¾åœ¨ /workspace/input/ ç›®å½•")
        return
    
    if not os.path.exists(golden_text_file):
        print(f"âŒ é»„é‡‘æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {golden_text_file}")
        print("è¯·å°† text_data_output.csv æ–‡ä»¶æ”¾åœ¨ /workspace/input/ ç›®å½•")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_directory, exist_ok=True)
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒwavå’Œmp3æ ¼å¼)
    wav_files = list(Path(audio_directory).glob("*.wav"))
    mp3_files = list(Path(audio_directory).glob("*.mp3"))
    audio_files = wav_files + mp3_files
    
    if not audio_files:
        print(f"âŒ åœ¨ {audio_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶(.wav æˆ– .mp3)")
        return
    
    print(f"âœ… æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {len(wav_files)} ä¸ª.wav, {len(mp3_files)} ä¸ª.mp3 (æ€»è®¡ {len(audio_files)} ä¸ª)")
    
    logger.info("=== å¼€å§‹æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶ ===")
    
    # å¼€å§‹å¤„ç†
    process_conversations_with_golden_text(
        audio_dir=audio_directory,
        golden_text_path=golden_text_file,
        output_dir=output_directory,
        conversation_mapping=None  # ä½¿ç”¨è‡ªåŠ¨è§£æ
    )
    
    logger.info("=== å¤„ç†å®Œæˆ ===")
    print("ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨ /workspace/output/")


if __name__ == "__main__":
    main()
