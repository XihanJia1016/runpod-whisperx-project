"""
RunPod WhisperX Large-v3 é«˜ç²¾åº¦éŸ³é¢‘å¤„ç†è„šæœ¬
åŠè‡ªåŠ¨ç§å­è¯†åˆ«è¯´è¯äººæ—¥å¿—ç³»ç»Ÿ
ä¼˜åŒ–è¯´è¯äººè¯†åˆ«å’Œæ—¶é—´æˆ³ç²¾åº¦
"""

import whisperx
import pandas as pd
import torch
import gc
import os
import time
import logging
from pathlib import Path
import numpy as np
from pyannote.audio import Pipeline
import subprocess
import shutil
import difflib
from sklearn.metrics.pairwise import cosine_similarity

# ç¯å¢ƒæ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤
def check_and_fix_environment():
    """æ£€æŸ¥ç¯å¢ƒå¹¶åœ¨éœ€è¦æ—¶è‡ªåŠ¨ä¿®å¤"""
    
    missing_packages = []
    version_conflicts = []
    
    try:
        import torch
        import torchvision
        
        torch_version = torch.__version__
        torchvision_version = torchvision.__version__
        
        # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
        if not (torch_version.startswith('2.1.') and torchvision_version.startswith('0.16.')):
            version_conflicts.append(f"PyTorchç‰ˆæœ¬å†²çª: torch={torch_version}, torchvision={torchvision_version}")
    except ImportError as e:
        missing_packages.append(f"PyTorch: {e}")
    
    try:
        import pyannote.audio
    except ImportError as e:
        missing_packages.append(f"pyannote.audio: {e}")
    
    try:
        import whisperx
    except ImportError as e:
        missing_packages.append(f"whisperx: {e}")
    
    try:
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError as e:
        missing_packages.append(f"scikit-learn: {e}")
    
    if missing_packages or version_conflicts:
        print("âŒ ç¯å¢ƒæ£€æŸ¥å‘ç°é—®é¢˜:")
        for issue in missing_packages + version_conflicts:
            print(f"  - {issue}")
        
        print("\nğŸ”§ è‡ªåŠ¨ä¿®å¤å»ºè®®:")
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿®å¤ç¯å¢ƒ:")
        print("  python setup_environment.py")
        print("\næˆ–è€…æ‰‹åŠ¨ä¿®å¤:")
        print("  pip install -r requirements_stable.txt")
        
        # è¯¢é—®æ˜¯å¦è‡ªåŠ¨ä¿®å¤
        response = input("\næ˜¯å¦ç°åœ¨è‡ªåŠ¨ä¿®å¤ï¼Ÿ(y/N): ")
        if response.lower() in ['y', 'yes']:
            print("ğŸ”„ å¼€å§‹è‡ªåŠ¨ä¿®å¤...")
            try:
                import setup_environment
                setup_environment.main()
                print("âœ… ç¯å¢ƒä¿®å¤å®Œæˆï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬")
                exit(0)
            except Exception as e:
                print(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨è¿è¡Œ: python setup_environment.py")
                exit(1)
        else:
            print("è¯·å…ˆä¿®å¤ç¯å¢ƒé—®é¢˜åå†è¿è¡Œè„šæœ¬")
            exit(1)
    else:
        print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")

# ç§»é™¤è‡ªåŠ¨ç¯å¢ƒæ£€æŸ¥ï¼Œé¿å…åœ¨RunPodä¸­å¡ä½
# check_and_fix_environment()  # å·²ç¦ç”¨ï¼Œé˜²æ­¢äº¤äº’å¼è¾“å…¥å¯¼è‡´å¡æ­»

# ç¦ç”¨TF32ä»¥é¿å…ç²¾åº¦å’Œå…¼å®¹æ€§é—®é¢˜
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_cudnn_issue():
    """ä¿®å¤cuDNNåº“é—®é¢˜"""
    logger.info("ğŸ”§ å°è¯•ä¿®å¤cuDNNåº“é—®é¢˜...")
    
    try:
        # å°è¯•å®‰è£…ç¼ºå¤±çš„cuDNNåº“
        logger.info("ğŸ”§ å®‰è£…cuDNNåº“...")
        subprocess.run(['apt', 'update'], check=True, capture_output=True)
        subprocess.run(['apt', 'install', '-y', 'libcudnn8', 'libcudnn8-dev'], check=True, capture_output=True)
        logger.info("âœ… cuDNNåº“å®‰è£…å®Œæˆ")
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        logger.warning(f"âš ï¸ æ— æ³•é€šè¿‡aptå®‰è£…cuDNN: {e}")
        
        # å°è¯•ä½¿ç”¨condaå®‰è£…
        try:
            logger.info("ğŸ”§ å°è¯•é€šè¿‡condaå®‰è£…cudnn...")
            subprocess.run(['conda', 'install', '-c', 'conda-forge', 'cudnn', '-y'], check=True, capture_output=True)
            logger.info("âœ… cuDNNé€šè¿‡condaå®‰è£…å®Œæˆ")
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.warning(f"âš ï¸ condaå®‰è£…ä¹Ÿå¤±è´¥: {e}")

def setup_cuda_environment():
    """è®¾ç½®CUDAç¯å¢ƒå˜é‡ä¿®å¤cuDNNé—®é¢˜"""
    # å…ˆå°è¯•ä¿®å¤cuDNN
    fix_cudnn_issue()
    
    cuda_paths = [
        "/usr/local/cuda/lib64",
        "/usr/local/cuda-11.8/lib64", 
        "/usr/local/cuda-11.7/lib64",
        "/usr/local/cuda-11.6/lib64",
        "/usr/lib/x86_64-linux-gnu",
        "/opt/conda/lib",
        "/opt/conda/pkgs/cudnn*/lib",
        "/usr/local/lib/python3.10/dist-packages/nvidia/cudnn/lib"
    ]
    
    # æ£€æŸ¥PyTorchä¿¡æ¯
    logger.info(f"ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"ğŸ”§ cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # è®¾ç½®LD_LIBRARY_PATH
    current_path = os.environ.get('LD_LIBRARY_PATH', '')
    new_paths = []
    
    for path in cuda_paths:
        if os.path.exists(path):
            new_paths.append(path)
            logger.info(f"âœ… æ‰¾åˆ°CUDAåº“è·¯å¾„: {path}")
    
    if new_paths:
        if current_path:
            os.environ['LD_LIBRARY_PATH'] = ':'.join(new_paths) + ':' + current_path
        else:
            os.environ['LD_LIBRARY_PATH'] = ':'.join(new_paths)
        logger.info(f"ğŸ”§ å·²è®¾ç½®LD_LIBRARY_PATH")
    
    # è®¾ç½®é¢å¤–çš„ç¯å¢ƒå˜é‡æ¥ç¨³å®šcuDNN
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    
    # å¼ºåˆ¶é‡æ–°åŠ è½½åŠ¨æ€åº“
    try:
        import ctypes
        ctypes.CDLL("libcudnn.so.8", mode=ctypes.RTLD_GLOBAL)
        logger.info("âœ… æˆåŠŸåŠ è½½libcudnn.so.8")
    except Exception as e:
        logger.warning(f"âš ï¸ æ— æ³•åŠ è½½libcudnn.so.8: {e}")
    
def ensure_ffmpeg():
    """ç¡®ä¿ffmpegå·²å®‰è£…"""
    if shutil.which('ffmpeg') is None:
        logger.info("ğŸ”§ æœªæ‰¾åˆ°ffmpegï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
        try:
            # å°è¯•ä½¿ç”¨aptå®‰è£…
            subprocess.run(['apt', 'update'], check=True, capture_output=True)
            subprocess.run(['apt', 'install', '-y', 'ffmpeg'], check=True, capture_output=True)
            logger.info("âœ… ffmpegå®‰è£…æˆåŠŸ")
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                # å¦‚æœaptå¤±è´¥ï¼Œå°è¯•conda
                subprocess.run(['conda', 'install', '-c', 'conda-forge', 'ffmpeg', '-y'], check=True, capture_output=True)
                logger.info("âœ… ffmpegé€šè¿‡condaå®‰è£…æˆåŠŸ")
            except (subprocess.CalledProcessError, FileNotFoundError):
                logger.error("âŒ æ— æ³•è‡ªåŠ¨å®‰è£…ffmpegï¼Œè¯·æ‰‹åŠ¨å®‰è£…")
                logger.info("ğŸ’¡ æ‰‹åŠ¨å®‰è£…å‘½ä»¤: apt install -y ffmpeg æˆ– conda install -c conda-forge ffmpeg")
                raise RuntimeError("ffmpegæœªå®‰è£…ä¸”æ— æ³•è‡ªåŠ¨å®‰è£…")
    else:
        logger.info("âœ… ffmpegå·²å®‰è£…")

class HighPrecisionAudioProcessor:
    def __init__(self):
        # è®¾ç½®CUDAç¯å¢ƒä¿®å¤cuDNNé—®é¢˜
        setup_cuda_environment()
        # ç¡®ä¿ffmpegå·²å®‰è£…
        ensure_ffmpeg()
        self.device = self._setup_device()
        self.model = None
        self.align_model = None
        self.metadata = None
        self.diarize_model = None
        self.embedding_model = None
        
        # é«˜ç²¾åº¦é…ç½®
        self.config = {
            'model_size': 'large-v3',
            'batch_size': 8,  # GPUä¼˜åŒ–
            'chunk_length': 30,  # 30ç§’å—ï¼Œå¹³è¡¡ç²¾åº¦å’Œå†…å­˜
            'return_attention': True,
            'word_timestamps': True,
            'vad_filter': True,
            'temperature': 0.0,  # ç¡®å®šæ€§è¾“å‡º
            'enable_speaker_diarization': True,  # å¯ç”¨è¯´è¯äººè¯†åˆ«
        }
        
        logger.info(f"åˆå§‹åŒ–é«˜ç²¾åº¦å¤„ç†å™¨: è®¾å¤‡={self.device}")
    
    def _setup_device(self):
        if torch.cuda.is_available():
            device = "cuda"
            logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            device = "cpu"
            logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        return device
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰å¿…éœ€æ¨¡å‹"""
        try:
            # 1. åŠ è½½Whisper Large-v3
            logger.info("åŠ è½½Whisper Large-v3æ¨¡å‹...")
            self.model = whisperx.load_model(
                self.config['model_size'], 
                device=self.device,
                compute_type="float16",
                download_root="/workspace/cache"
            )
            logger.info("âœ… Whisper Large-v3åŠ è½½å®Œæˆ")
            
            # 2. åŠ è½½å¯¹é½æ¨¡å‹ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            logger.info("åŠ è½½å¼ºåˆ¶å¯¹é½æ¨¡å‹...")
            self.align_model, self.metadata = whisperx.load_align_model(
                language_code="nl",  # è·å…°è¯­
                device=self.device
            )
            logger.info("âœ… å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # 3. åŠ è½½è¯´è¯äººåµŒå…¥æ¨¡å‹ (æ›¿æ¢è¯´è¯äººè¯†åˆ«æ¨¡å‹)
            logger.info("åŠ è½½è¯´è¯äººåµŒå…¥æ¨¡å‹...")
            # è·å–HuggingFace token (éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ HF_TOKEN)
            hf_token = os.getenv('HF_TOKEN')
            if not hf_token:
                logger.warning("âš ï¸ æœªè®¾ç½®HF_TOKENç¯å¢ƒå˜é‡ï¼Œè¯´è¯äººåµŒå…¥å¯èƒ½å¤±è´¥")
                logger.info("ğŸ’¡ è¯·è¿è¡Œ: export HF_TOKEN='your_token_here'")
            
            # æ³¨é‡Šæ‰åŸæ¥çš„diarizationæ¨¡å‹
            # self.diarize_model = Pipeline.from_pretrained(
            #     "pyannote/speaker-diarization-3.1", 
            #     use_auth_token=hf_token
            # )
            
            # åŠ è½½åµŒå…¥æ¨¡å‹ç”¨äºç§å­è¯†åˆ«
            try:
                logger.info("â³ æ­£åœ¨ä¸‹è½½å’ŒåŠ è½½ pyannote/embedding æ¨¡å‹...")
                self.embedding_model = Pipeline.from_pretrained(
                    "pyannote/embedding",
                    use_auth_token=hf_token
                )
                
                if self.embedding_model is None:
                    raise ValueError("åµŒå…¥æ¨¡å‹åŠ è½½è¿”å›None")
                    
                # ç§»åŠ¨åˆ°è®¾å¤‡
                self.embedding_model = self.embedding_model.to(self.device)
                logger.info("âœ… è¯´è¯äººåµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
                
            except Exception as e:
                logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.info("ğŸ”„ å°è¯•æ¸…é™¤ç¼“å­˜åé‡æ–°åŠ è½½...")
                
                # æ¸…é™¤å¯èƒ½æŸåçš„ç¼“å­˜
                import shutil
                cache_dir = os.path.expanduser("~/.cache/huggingface/transformers")
                if os.path.exists(cache_dir):
                    try:
                        shutil.rmtree(cache_dir)
                        logger.info("âœ… ç¼“å­˜æ¸…é™¤å®Œæˆ")
                    except:
                        pass
                
                # é‡æ–°å°è¯•åŠ è½½
                try:
                    self.embedding_model = Pipeline.from_pretrained(
                        "pyannote/embedding",
                        use_auth_token=hf_token,
                        cache_dir="/tmp/huggingface_cache"  # ä½¿ç”¨ä¸´æ—¶ç›®å½•
                    ).to(self.device)
                    logger.info("âœ… è¯´è¯äººåµŒå…¥æ¨¡å‹é‡æ–°åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    logger.error(f"é‡æ–°åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                    raise e2
            
            # å°†åŸæ¥çš„diarize_modelè®¾ä¸ºNone
            self.diarize_model = None
            
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _seconds_to_timestamp(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _clean_text_for_comparison(self, text):
        """æ¸…ç†æ–‡æœ¬ä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œæ¯”è¾ƒ"""
        if pd.isna(text):
            return ""
        import re
        return re.sub(r'\s+', ' ', str(text).strip().lower())
    
    def _find_seed_segments(self, golden_turns_df, ai_segments):
        """
        æ ¹æ®é»„é‡‘æ–‡æœ¬ï¼Œä»AIè½¬å½•ç‰‡æ®µä¸­æ‰¾åˆ°æ¯ä¸ªè¯´è¯äººçš„ç§å­ç‰‡æ®µ
        
        Args:
            golden_turns_df: å·²ç­›é€‰çš„é»„é‡‘æ–‡æœ¬DataFrame (å½“å‰dyadå’Œconversation)
            ai_segments: å½“å‰å¯¹è¯çš„æ‰€æœ‰AIè½¬å½•ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            dict: {'S': [segment1, segment2, ...], 'L': [segment3, segment4, ...]}
        """
        logger.info("å¼€å§‹å¯»æ‰¾è¯´è¯äººç§å­ç‰‡æ®µ...")
        
        seed_map = {'S': [], 'L': []}
        
        try:
            # æ‰¾åˆ°Så’ŒLçš„ç¬¬ä¸€ä¸ªç›®æ ‡æ–‡æœ¬
            s_turns = golden_turns_df[golden_turns_df['role'] == 'S']
            l_turns = golden_turns_df[golden_turns_df['role'] == 'L']
            
            if s_turns.empty or l_turns.empty:
                logger.warning("æœªæ‰¾åˆ°Sæˆ–Lçš„é»„é‡‘æ–‡æœ¬ï¼Œæ— æ³•ç”Ÿæˆç§å­")
                return seed_map
            
            # è·å–ç¬¬ä¸€ä¸ªç›®æ ‡æ–‡æœ¬
            s_target_text = self._clean_text_for_comparison(s_turns.iloc[0]['text'])
            l_target_text = self._clean_text_for_comparison(l_turns.iloc[0]['text'])
            
            logger.info(f"Sç›®æ ‡æ–‡æœ¬: {s_target_text[:50]}...")
            logger.info(f"Lç›®æ ‡æ–‡æœ¬: {l_target_text[:50]}...")
            
            # ä¸ºSæ‰¾ç§å­ç‰‡æ®µ
            s_seed_segments = self._find_best_matching_segments(s_target_text, ai_segments)
            if s_seed_segments:
                seed_map['S'] = s_seed_segments
                logger.info(f"æ‰¾åˆ°Sçš„ç§å­ç‰‡æ®µ: {len(s_seed_segments)}ä¸ª")
            
            # ä¸ºLæ‰¾ç§å­ç‰‡æ®µ (æ’é™¤å·²ç”¨äºSçš„ç‰‡æ®µ)
            remaining_segments = [seg for seg in ai_segments if seg not in s_seed_segments]
            l_seed_segments = self._find_best_matching_segments(l_target_text, remaining_segments)
            if l_seed_segments:
                seed_map['L'] = l_seed_segments
                logger.info(f"æ‰¾åˆ°Lçš„ç§å­ç‰‡æ®µ: {len(l_seed_segments)}ä¸ª")
            
            return seed_map
            
        except Exception as e:
            logger.error(f"å¯»æ‰¾ç§å­ç‰‡æ®µå¤±è´¥: {e}")
            return {'S': [], 'L': []}
    
    def _find_best_matching_segments(self, target_text, ai_segments):
        """
        ä½¿ç”¨è´ªå¿ƒç®—æ³•æ‰¾åˆ°ä¸ç›®æ ‡æ–‡æœ¬æœ€åŒ¹é…çš„AIç‰‡æ®µç»„åˆ
        
        Args:
            target_text: æ¸…ç†åçš„ç›®æ ‡æ–‡æœ¬
            ai_segments: å¯ç”¨çš„AIç‰‡æ®µåˆ—è¡¨
            
        Returns:
            list: æœ€ä½³åŒ¹é…çš„ç‰‡æ®µåˆ—è¡¨
        """
        if not target_text or not ai_segments:
            return []
        
        best_match_ratio = 0
        best_match_segments = []
        
        # è´ªå¿ƒæœç´¢ï¼šå°è¯•ä¸åŒçš„ç‰‡æ®µç»„åˆ
        for start_idx in range(len(ai_segments)):
            temp_text = ""
            temp_segments = []
            
            for end_idx in range(start_idx, min(start_idx + 5, len(ai_segments))):  # æœ€å¤šç»„åˆ5ä¸ªç‰‡æ®µ
                segment = ai_segments[end_idx]
                temp_segments.append(segment)
                temp_text += " " + self._clean_text_for_comparison(segment.get('text', ''))
                temp_text = temp_text.strip()
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                if temp_text:
                    similarity = difflib.SequenceMatcher(None, temp_text, target_text).ratio()
                    
                    if similarity > best_match_ratio:
                        best_match_ratio = similarity
                        best_match_segments = temp_segments.copy()
                    
                    # å¦‚æœç›¸ä¼¼åº¦å¼€å§‹ä¸‹é™ï¼Œæå‰åœæ­¢
                    if len(temp_segments) > 1 and similarity < best_match_ratio * 0.8:
                        break
        
        logger.info(f"æœ€ä½³åŒ¹é…ç›¸ä¼¼åº¦: {best_match_ratio:.3f}")
        return best_match_segments
    
    def perform_seed_based_diarization(self, audio_data, all_ai_segments, seed_map):
        """
        åŸºäºç§å­ç‰‡æ®µè¿›è¡Œè¯´è¯äººè¯†åˆ«çš„æ ¸å¿ƒå‡½æ•°
        
        Args:
            audio_data: whisperx.load_audio()è¿”å›çš„numpyæ•°ç»„
            all_ai_segments: å½“å‰å¯¹è¯çš„æ‰€æœ‰AIè½¬å½•ç‰‡æ®µ
            seed_map: ç§å­å­—å…¸ {'S': [...], 'L': [...]}
            
        Returns:
            list: æ›´æ–°äº†speakerå­—æ®µçš„all_ai_segments
        """
        logger.info("å¼€å§‹åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«...")
        
        try:
            # 1. ç”Ÿæˆç§å­æŒ‡çº¹
            s_seed_embedding = self._generate_seed_embedding(audio_data, seed_map['S'])
            l_seed_embedding = self._generate_seed_embedding(audio_data, seed_map['L'])
            
            if s_seed_embedding is None or l_seed_embedding is None:
                logger.error("æ— æ³•ç”Ÿæˆç§å­åµŒå…¥ï¼Œè·³è¿‡è¯´è¯äººè¯†åˆ«")
                return all_ai_segments
            
            logger.info("âœ… ç§å­åµŒå…¥ç”Ÿæˆå®Œæˆ")
            
            # 2. è¯†åˆ«æ‰€æœ‰ç‰‡æ®µ
            sample_rate = 16000  # WhisperXä½¿ç”¨16kHz
            
            for i, segment in enumerate(all_ai_segments):
                try:
                    # æå–éŸ³é¢‘ç‰‡æ®µ
                    start_sample = int(segment.get('start', 0) * sample_rate)
                    end_sample = int(segment.get('end', 0) * sample_rate)
                    
                    # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    start_sample = max(0, start_sample)
                    end_sample = min(len(audio_data), end_sample)
                    
                    if start_sample >= end_sample:
                        logger.warning(f"ç‰‡æ®µ{i}æ—¶é—´æˆ³æ— æ•ˆï¼Œè·³è¿‡")
                        segment['speaker'] = 'UNKNOWN'
                        continue
                    
                    audio_segment = audio_data[start_sample:end_sample]
                    
                    # ç”Ÿæˆç‰‡æ®µåµŒå…¥
                    segment_embedding = self._generate_single_embedding(audio_segment)
                    
                    if segment_embedding is not None:
                        # è®¡ç®—ä¸ç§å­çš„ç›¸ä¼¼åº¦
                        s_similarity = cosine_similarity(
                            segment_embedding.reshape(1, -1), 
                            s_seed_embedding.reshape(1, -1)
                        )[0][0]
                        
                        l_similarity = cosine_similarity(
                            segment_embedding.reshape(1, -1), 
                            l_seed_embedding.reshape(1, -1)
                        )[0][0]
                        
                        # åˆ†é…è¯´è¯äºº
                        if s_similarity > l_similarity:
                            segment['speaker'] = 'S'
                        else:
                            segment['speaker'] = 'L'
                        
                        # å¯é€‰ï¼šè®°å½•ç½®ä¿¡åº¦
                        segment['speaker_confidence'] = max(s_similarity, l_similarity)
                        
                    else:
                        logger.warning(f"ç‰‡æ®µ{i}æ— æ³•ç”ŸæˆåµŒå…¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡è®°")
                        segment['speaker'] = 'UNKNOWN'
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†ç‰‡æ®µ{i}å¤±è´¥: {e}")
                    segment['speaker'] = 'UNKNOWN'
            
            # ç»Ÿè®¡ç»“æœ
            s_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'S')
            l_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'L')
            unknown_count = sum(1 for seg in all_ai_segments if seg.get('speaker') == 'UNKNOWN')
            
            logger.info(f"è¯´è¯äººè¯†åˆ«ç»“æœ: S={s_count}, L={l_count}, Unknown={unknown_count}")
            
            return all_ai_segments
            
        except Exception as e:
            logger.error(f"åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«å¤±è´¥: {e}")
            return all_ai_segments
    
    def _generate_seed_embedding(self, audio_data, seed_segments):
        """
        ä¸ºç§å­ç‰‡æ®µç”Ÿæˆå¹³å‡åµŒå…¥å‘é‡
        
        Args:
            audio_data: å®Œæ•´éŸ³é¢‘æ•°æ®
            seed_segments: ç§å­ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            numpy.ndarray: å¹³å‡åµŒå…¥å‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        if not seed_segments:
            return None
        
        sample_rate = 16000
        embeddings = []
        
        for segment in seed_segments:
            try:
                start_sample = int(segment.get('start', 0) * sample_rate)
                end_sample = int(segment.get('end', 0) * sample_rate)
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                start_sample = max(0, start_sample)
                end_sample = min(len(audio_data), end_sample)
                
                if start_sample >= end_sample:
                    continue
                
                audio_segment = audio_data[start_sample:end_sample]
                embedding = self._generate_single_embedding(audio_segment)
                
                if embedding is not None:
                    embeddings.append(embedding)
                    
            except Exception as e:
                logger.warning(f"ç§å­ç‰‡æ®µåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                continue
        
        if embeddings:
            # è®¡ç®—å¹³å‡åµŒå…¥
            mean_embedding = np.mean(embeddings, axis=0)
            return mean_embedding
        else:
            return None
    
    def _generate_single_embedding(self, audio_segment):
        """
        ä¸ºå•ä¸ªéŸ³é¢‘ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            audio_segment: éŸ³é¢‘ç‰‡æ®µ (numpy array)
            
        Returns:
            numpy.ndarray: åµŒå…¥å‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ (è‡³å°‘0.1ç§’)
            min_length = int(0.1 * 16000)
            if len(audio_segment) < min_length:
                # å¦‚æœå¤ªçŸ­ï¼Œç”¨é›¶å¡«å……
                audio_segment = np.pad(audio_segment, (0, min_length - len(audio_segment)))
            
            # è½¬æ¢ä¸ºPyTorch tensor
            audio_tensor = torch.from_numpy(audio_segment).float().unsqueeze(0)
            
            if self.device == "cuda":
                audio_tensor = audio_tensor.cuda()
            
            # ç”ŸæˆåµŒå…¥
            with torch.no_grad():
                embedding = self.embedding_model({
                    "waveform": audio_tensor, 
                    "sample_rate": 16000
                })
            
            # è½¬æ¢ä¸ºnumpy
            if isinstance(embedding, torch.Tensor):
                embedding = embedding.cpu().numpy()
            
            return embedding.flatten()
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆå•ä¸ªåµŒå…¥å¤±è´¥: {e}")
            return None
    
    def process_single_file(self, audio_path, dyad_id, conversation_id, golden_turns_df=None):
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¤„ç†: {Path(audio_path).name}")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½éŸ³é¢‘
            logger.info("åŠ è½½éŸ³é¢‘...")
            audio = whisperx.load_audio(audio_path)
            duration = len(audio) / 16000
            logger.info(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
            
            # 2. è½¬å½•ï¼ˆLarge-v3é«˜ç²¾åº¦ï¼‰
            logger.info("å¼€å§‹é«˜ç²¾åº¦è½¬å½•...")
            result = self.model.transcribe(
                audio,
                batch_size=self.config['batch_size']
            )
            
            segments = result.get("segments", [])
            logger.info(f"è½¬å½•å®Œæˆ: {len(segments)}ä¸ªç‰‡æ®µ")
            
            # 3. å¼ºåˆ¶å¯¹é½ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            if self.align_model and segments:
                logger.info("è¿›è¡Œå¼ºåˆ¶å¯¹é½...")
                result = whisperx.align(
                    result["segments"], 
                    self.align_model, 
                    self.metadata, 
                    audio, 
                    self.device, 
                    return_char_alignments=False
                )
                logger.info("âœ… å¼ºåˆ¶å¯¹é½å®Œæˆ")
            
            # 4. åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ« (æ–°æµç¨‹)
            speaker_success = False
            if self.embedding_model and golden_turns_df is not None:
                logger.info("å¼€å§‹åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«...")
                try:
                    # æ­¥éª¤A: æ‰¾åˆ°ç§å­ç‰‡æ®µ
                    seed_map = self._find_seed_segments(golden_turns_df, result["segments"])
                    
                    if seed_map.get('S') and seed_map.get('L'):
                        # æ­¥éª¤B: æ‰§è¡Œç§å­è¯†åˆ«
                        result["segments"] = self.perform_seed_based_diarization(
                            audio,  # ä¼ å…¥å·²åŠ è½½çš„audioæ•°æ®
                            result["segments"],
                            seed_map
                        )
                        speaker_success = True
                        logger.info("âœ… åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«å®Œæˆ")
                    else:
                        logger.warning(f"å¯¹è¯ {dyad_id}-{conversation_id}: æœªèƒ½æ‰¾åˆ°Så’ŒLçš„ç§å­ï¼Œå°†ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                        speaker_success = False
                        
                except Exception as e:
                    logger.error(f"âŒ å¯¹è¯ {dyad_id}-{conversation_id}: åŸºäºç§å­çš„è¯´è¯äººè¯†åˆ«å¤±è´¥: {e}")
                    speaker_success = False
            else:
                logger.warning("åµŒå…¥æ¨¡å‹æœªåŠ è½½æˆ–æœªæä¾›é»„é‡‘æ–‡æœ¬ï¼Œè·³è¿‡è¯´è¯äººè¯†åˆ«")
                speaker_success = False
            
            # 5. å¤„ç†å’Œæ ¼å¼åŒ–ç»“æœ
            processed_segments = self._format_results(
                result.get("segments", []), 
                dyad_id, 
                conversation_id, 
                speaker_success,
                duration
            )
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {processing_time:.1f}ç§’")
            
            # æ¸…ç†GPUå†…å­˜
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            
            return processed_segments
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _format_results(self, segments, dyad_id, conversation_id, speaker_success, total_duration):
        """æ ¼å¼åŒ–è½¬å½•ç»“æœ"""
        processed = []
        
        # è°ƒè¯•ï¼šè®°å½•ç¬¬ä¸€ä¸ªsegmentçš„ç»“æ„
        if segments and len(segments) > 0:
            logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªsegmentåŒ…å«çš„å­—æ®µ: {list(segments[0].keys())}")
        
        for i, seg in enumerate(segments):
            # å¤„ç†è¯´è¯äººä¿¡æ¯
            if speaker_success and "speaker" in seg:
                speaker_raw = seg["speaker"]
                # å¤„ç†æ–°çš„S/Læ ‡è¯†ç³»ç»Ÿ
                if speaker_raw == 'S':
                    speaker_name = "Speaker_A"
                    speaker_raw = "SPEAKER_00"
                elif speaker_raw == 'L':
                    speaker_name = "Speaker_B"
                    speaker_raw = "SPEAKER_01"
                elif "SPEAKER_00" in str(speaker_raw):
                    speaker_name = "Speaker_A"
                elif "SPEAKER_01" in str(speaker_raw):
                    speaker_name = "Speaker_B"
                else:
                    # å¤„ç†å…¶ä»–æƒ…å†µ
                    speaker_name = f"Speaker_{str(speaker_raw).split('_')[-1]}"
            else:
                # ç®€å•äº¤æ›¿åˆ†é…
                speaker_name = f"Speaker_{'A' if i % 2 == 0 else 'B'}"
                speaker_raw = f"SPEAKER_{i % 2:02d}"
            
            # è·å–æ—¶é—´æˆ³
            start_seconds = seg.get('start', 0)
            end_seconds = seg.get('end', 0)
            duration = end_seconds - start_seconds
            
            # è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
            start_time = self._seconds_to_timestamp(start_seconds)
            finish_time = self._seconds_to_timestamp(end_seconds)
            
            # è·å–æ–‡æœ¬å’Œç½®ä¿¡åº¦
            text = seg.get('text', '').strip()
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä»å¤šä¸ªå¯èƒ½çš„æºï¼‰
            confidence = 0.0
            if 'confidence' in seg:
                confidence = seg['confidence']
            elif 'avg_logprob' in seg:
                # å°†logprobè½¬æ¢ä¸ºç½®ä¿¡åº¦è¿‘ä¼¼å€¼
                confidence = min(1.0, max(0.0, (seg['avg_logprob'] + 1.0)))
            elif 'words' in seg and seg['words']:
                # ä»è¯çº§ç½®ä¿¡åº¦è®¡ç®—å¹³å‡å€¼
                word_confidences = [w.get('probability', 0.0) for w in seg['words'] if 'probability' in w]
                if word_confidences:
                    confidence = sum(word_confidences) / len(word_confidences)
                else:
                    # å¦‚æœæ²¡æœ‰ä»»ä½•ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œæ ¹æ®æ— åœé¡¿æ—¶é—´ä¼°ç®—
                    confidence = 0.85 if seg.get('no_speech_prob', 1.0) < 0.5 else 0.3
            else:
                # é»˜è®¤åˆç†ç½®ä¿¡åº¦ï¼ˆè€Œé0ï¼‰
                confidence = 0.8
            
            # ç»Ÿè®¡è¯çº§ä¿¡æ¯
            words = seg.get('words', [])
            word_count = len(text.split()) if text else 0
            
            processed.append({
                'dyad': dyad_id,
                'conversation': conversation_id,
                'segment_id': i + 1,
                'start_time': start_time,  # æ–°æ ¼å¼ï¼šHH:MM:SS,mmm
                'finish_time': finish_time,  # æ”¹åä¸ºfinish_time
                'duration': round(duration, 3),
                'speaker': speaker_name,
                'speaker_raw': speaker_raw,
                'text': text,
                'confidence': round(confidence, 4),
                'word_count': word_count,
                'language': 'nl',  # è·å…°è¯­
                'model_used': 'large-v3',
                'device_used': self.device,
                'has_ai_speaker_detection': speaker_success
            })
        
        return processed
    
    def cleanup(self):
        """æ¸…ç†æ¨¡å‹å’Œå†…å­˜"""
        if self.model:
            del self.model
        if self.align_model:
            del self.align_model
        if self.diarize_model:
            del self.diarize_model
        if self.embedding_model:
            del self.embedding_model
        
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("å†…å­˜æ¸…ç†å®Œæˆ")


def load_golden_text_data(golden_text_path):
    """åŠ è½½é»„é‡‘æ ‡å‡†æ–‡æœ¬æ•°æ®"""
    try:
        logger.info(f"åŠ è½½é»„é‡‘æ–‡æœ¬æ•°æ®: {golden_text_path}")
        df = pd.read_csv(golden_text_path)
        df.columns = df.columns.str.strip()  # æ¸…ç†åˆ—å
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['dyad', 'conversation', 'role', 'text']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.error(f"é»„é‡‘æ–‡æœ¬ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            logger.info(f"å¯ç”¨åˆ—: {list(df.columns)}")
            return None
        
        logger.info(f"âœ… é»„é‡‘æ–‡æœ¬åŠ è½½å®Œæˆ: {len(df)}è¡Œæ•°æ®")
        logger.info(f"åŒ…å«å¯¹è¯: {df['dyad'].nunique()}ä¸ªdyad, {df['conversation'].nunique()}ä¸ªconversation")
        
        return df
        
    except Exception as e:
        logger.error(f"åŠ è½½é»„é‡‘æ–‡æœ¬å¤±è´¥: {e}")
        return None


def process_conversations_with_golden_text(
    audio_dir, 
    golden_text_path, 
    output_dir, 
    conversation_mapping=None
):
    """
    ä½¿ç”¨é»„é‡‘æ–‡æœ¬æ•°æ®æ‰¹é‡å¤„ç†å¯¹è¯
    
    Args:
        audio_dir: éŸ³é¢‘æ–‡ä»¶ç›®å½•
        golden_text_path: é»„é‡‘æ–‡æœ¬CSVæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        conversation_mapping: éŸ³é¢‘æ–‡ä»¶ååˆ°(dyad, conversation)çš„æ˜ å°„å­—å…¸
                            å¦‚æœä¸ºNoneï¼Œå°†å°è¯•ä»æ–‡ä»¶åè§£æ
    """
    
    # åŠ è½½é»„é‡‘æ–‡æœ¬
    golden_df = load_golden_text_data(golden_text_path)
    if golden_df is None:
        logger.error("æ— æ³•åŠ è½½é»„é‡‘æ–‡æœ¬ï¼Œç»ˆæ­¢å¤„ç†")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = HighPrecisionAudioProcessor()
    
    try:
        # åŠ è½½æ¨¡å‹
        if not processor.load_models():
            logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # è·å–éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        audio_files = []
        for ext in ['.wav', '.mp3', '.m4a', '.flac']:
            audio_files.extend(Path(audio_dir).glob(f"*{ext}"))
        
        logger.info(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        all_results = []
        processed_count = 0
        
        for audio_file in audio_files:
            try:
                # è§£ædyadå’Œconversation
                if conversation_mapping:
                    if audio_file.name in conversation_mapping:
                        dyad_id, conversation_id = conversation_mapping[audio_file.name]
                    else:
                        logger.warning(f"æ–‡ä»¶ {audio_file.name} ä¸åœ¨æ˜ å°„ä¸­ï¼Œè·³è¿‡")
                        continue
                else:
                    # å°è¯•ä»æ–‡ä»¶åè§£æ (å‡è®¾æ ¼å¼ä¸º dyad_X_conversation_Y.wav/.mp3)
                    try:
                        parts = audio_file.stem.split('_')
                        dyad_id = int(parts[1])
                        conversation_id = int(parts[3])
                    except (ValueError, IndexError):
                        logger.warning(f"æ— æ³•ä»æ–‡ä»¶åè§£ædyadå’Œconversation: {audio_file.name}")
                        continue
                
                # è¿‡æ»¤å¯¹åº”çš„é»„é‡‘æ–‡æœ¬
                golden_turns_df = golden_df[
                    (golden_df['dyad'] == dyad_id) & 
                    (golden_df['conversation'] == conversation_id)
                ].copy()
                
                if golden_turns_df.empty:
                    logger.warning(f"å¯¹è¯ {dyad_id}-{conversation_id} æ²¡æœ‰å¯¹åº”çš„é»„é‡‘æ–‡æœ¬ï¼Œè·³è¿‡")
                    continue
                
                logger.info(f"å¤„ç†å¯¹è¯ {dyad_id}-{conversation_id}: {audio_file.name}")
                logger.info(f"é»„é‡‘æ–‡æœ¬è½®æ¬¡: {len(golden_turns_df)}")
                
                # å¤„ç†éŸ³é¢‘æ–‡ä»¶
                segments = processor.process_single_file(
                    str(audio_file), 
                    dyad_id, 
                    conversation_id, 
                    golden_turns_df
                )
                
                all_results.extend(segments)
                processed_count += 1
                
                logger.info(f"âœ… å®Œæˆå¤„ç† {dyad_id}-{conversation_id}: {len(segments)}ä¸ªç‰‡æ®µ")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {audio_file.name} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        if all_results:
            output_file = os.path.join(output_dir, "combined_transcription_with_seed_diarization.csv")
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(output_file, index=False, encoding='utf-8')
            
            logger.info(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"æ€»è®¡å¤„ç†: {processed_count}ä¸ªå¯¹è¯, {len(all_results)}ä¸ªç‰‡æ®µ")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            if 'has_ai_speaker_detection' in results_df.columns:
                success_count = results_df['has_ai_speaker_detection'].sum()
                logger.info(f"è¯´è¯äººè¯†åˆ«æˆåŠŸç‡: {success_count}/{processed_count} ({success_count/processed_count*100:.1f}%)")
        else:
            logger.warning("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
    
    finally:
        # æ¸…ç†
        processor.cleanup()


def main():
    """ä¸»å‡½æ•° - RunPodä½¿ç”¨ç¤ºä¾‹"""
    
    print("ğŸš€ WhisperXåŠè‡ªåŠ¨ç§å­è¯†åˆ«è¯´è¯äººæ—¥å¿—ç³»ç»Ÿ")
    print("=" * 50)
    
    # è·³è¿‡è€—æ—¶çš„ç¯å¢ƒæ£€æŸ¥ï¼Œç›´æ¥å¼€å§‹
    print("âš¡ å¼€å§‹å¤„ç†...")
    
    # é»˜è®¤RunPodè·¯å¾„é…ç½®
    audio_directory = "/workspace/input"
    golden_text_file = "/workspace/input/text_data_output.csv"
    output_directory = "/workspace/output"
    
    print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶ç›®å½•: {audio_directory}")
    print(f"ğŸ“„ é»„é‡‘æ–‡æœ¬æ–‡ä»¶: {golden_text_file}")
    print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {output_directory}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶å’Œç›®å½•
    if not os.path.exists(audio_directory):
        print(f"âŒ éŸ³é¢‘ç›®å½•ä¸å­˜åœ¨: {audio_directory}")
        print("è¯·å°†éŸ³é¢‘æ–‡ä»¶(.wav æˆ– .mp3)æ”¾åœ¨ /workspace/input/ ç›®å½•")
        return
    
    if not os.path.exists(golden_text_file):
        print(f"âŒ é»„é‡‘æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {golden_text_file}")
        print("è¯·å°† text_data_output.csv æ–‡ä»¶æ”¾åœ¨ /workspace/input/ ç›®å½•")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_directory, exist_ok=True)
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒwavå’Œmp3æ ¼å¼)
    wav_files = list(Path(audio_directory).glob("*.wav"))
    mp3_files = list(Path(audio_directory).glob("*.mp3"))
    audio_files = wav_files + mp3_files
    
    if not audio_files:
        print(f"âŒ åœ¨ {audio_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶(.wav æˆ– .mp3)")
        return
    
    print(f"âœ… æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {len(wav_files)} ä¸ª.wav, {len(mp3_files)} ä¸ª.mp3 (æ€»è®¡ {len(audio_files)} ä¸ª)")
    
    logger.info("=== å¼€å§‹æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶ ===")
    
    # å¼€å§‹å¤„ç†
    process_conversations_with_golden_text(
        audio_dir=audio_directory,
        golden_text_path=golden_text_file,
        output_dir=output_directory,
        conversation_mapping=None  # ä½¿ç”¨è‡ªåŠ¨è§£æ
    )
    
    logger.info("=== å¤„ç†å®Œæˆ ===")
    print("ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨ /workspace/output/")


if __name__ == "__main__":
    main()
