"""
RunPod WhisperX Large-v3 é«˜ç²¾åº¦éŸ³é¢‘å¤„ç†è„šæœ¬
ä¼˜åŒ–è¯´è¯äººè¯†åˆ«å’Œæ—¶é—´æˆ³ç²¾åº¦
"""

import whisperx
import pandas as pd
import torch
import gc
import os
import time
import logging
from pathlib import Path
import numpy as np
from pyannote.audio import Pipeline
import subprocess
import shutil

# ç¦ç”¨TF32ä»¥é¿å…ç²¾åº¦å’Œå…¼å®¹æ€§é—®é¢˜
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def ensure_ffmpeg():
    """ç¡®ä¿ffmpegå·²å®‰è£…"""
    if shutil.which('ffmpeg') is None:
        logger.info("ğŸ”§ æœªæ‰¾åˆ°ffmpegï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
        try:
            # å°è¯•ä½¿ç”¨aptå®‰è£…
            subprocess.run(['apt', 'update'], check=True, capture_output=True)
            subprocess.run(['apt', 'install', '-y', 'ffmpeg'], check=True, capture_output=True)
            logger.info("âœ… ffmpegå®‰è£…æˆåŠŸ")
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                # å¦‚æœaptå¤±è´¥ï¼Œå°è¯•conda
                subprocess.run(['conda', 'install', '-c', 'conda-forge', 'ffmpeg', '-y'], check=True, capture_output=True)
                logger.info("âœ… ffmpegé€šè¿‡condaå®‰è£…æˆåŠŸ")
            except (subprocess.CalledProcessError, FileNotFoundError):
                logger.error("âŒ æ— æ³•è‡ªåŠ¨å®‰è£…ffmpegï¼Œè¯·æ‰‹åŠ¨å®‰è£…")
                logger.info("ğŸ’¡ æ‰‹åŠ¨å®‰è£…å‘½ä»¤: apt install -y ffmpeg æˆ– conda install -c conda-forge ffmpeg")
                raise RuntimeError("ffmpegæœªå®‰è£…ä¸”æ— æ³•è‡ªåŠ¨å®‰è£…")
    else:
        logger.info("âœ… ffmpegå·²å®‰è£…")

class HighPrecisionAudioProcessor:
    def __init__(self):
        # ç¡®ä¿ffmpegå·²å®‰è£…
        ensure_ffmpeg()
        self.device = self._setup_device()
        self.model = None
        self.align_model = None
        self.metadata = None
        self.diarize_model = None
        
        # é«˜ç²¾åº¦é…ç½®
        self.config = {
            'model_size': 'large-v3',
            'batch_size': 8,  # GPUä¼˜åŒ–
            'chunk_length': 30,  # 30ç§’å—ï¼Œå¹³è¡¡ç²¾åº¦å’Œå†…å­˜
            'return_attention': True,
            'word_timestamps': True,
            'vad_filter': True,
            'temperature': 0.0,  # ç¡®å®šæ€§è¾“å‡º
        }
        
        logger.info(f"åˆå§‹åŒ–é«˜ç²¾åº¦å¤„ç†å™¨: è®¾å¤‡={self.device}")
    
    def _setup_device(self):
        if torch.cuda.is_available():
            device = "cuda"
            logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            device = "cpu"
            logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
        return device
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰å¿…éœ€æ¨¡å‹"""
        try:
            # 1. åŠ è½½Whisper Large-v3
            logger.info("åŠ è½½Whisper Large-v3æ¨¡å‹...")
            self.model = whisperx.load_model(
                self.config['model_size'], 
                device=self.device,
                compute_type="float16",
                download_root="/workspace/cache"
            )
            logger.info("âœ… Whisper Large-v3åŠ è½½å®Œæˆ")
            
            # 2. åŠ è½½å¯¹é½æ¨¡å‹ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            logger.info("åŠ è½½å¼ºåˆ¶å¯¹é½æ¨¡å‹...")
            self.align_model, self.metadata = whisperx.load_align_model(
                language_code="nl",  # è·å…°è¯­
                device=self.device
            )
            logger.info("âœ… å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # 3. åŠ è½½è¯´è¯äººè¯†åˆ«æ¨¡å‹
            logger.info("åŠ è½½è¯´è¯äººè¯†åˆ«æ¨¡å‹...")
            # è·å–HuggingFace token (éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ HF_TOKEN)
            hf_token = os.getenv('HF_TOKEN')
            if not hf_token:
                logger.warning("âš ï¸ æœªè®¾ç½®HF_TOKENç¯å¢ƒå˜é‡ï¼Œè¯´è¯äººè¯†åˆ«å¯èƒ½å¤±è´¥")
                logger.info("ğŸ’¡ è¯·è¿è¡Œ: export HF_TOKEN='your_token_here'")
            
            self.diarize_model = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1", 
                use_auth_token=hf_token
            )
            if self.device == "cuda":
                self.diarize_model.to(torch.device("cuda"))
            logger.info("âœ… è¯´è¯äººè¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _seconds_to_timestamp(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS,mmm æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def process_single_file(self, audio_path, dyad_id, conversation_id):
        """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¤„ç†: {Path(audio_path).name}")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½éŸ³é¢‘
            logger.info("åŠ è½½éŸ³é¢‘...")
            audio = whisperx.load_audio(audio_path)
            duration = len(audio) / 16000
            logger.info(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
            
            # 2. è½¬å½•ï¼ˆLarge-v3é«˜ç²¾åº¦ï¼‰
            logger.info("å¼€å§‹é«˜ç²¾åº¦è½¬å½•...")
            result = self.model.transcribe(
                audio,
                batch_size=self.config['batch_size']
            )
            
            segments = result.get("segments", [])
            logger.info(f"è½¬å½•å®Œæˆ: {len(segments)}ä¸ªç‰‡æ®µ")
            
            # 3. å¼ºåˆ¶å¯¹é½ï¼ˆæé«˜æ—¶é—´æˆ³ç²¾åº¦ï¼‰
            if self.align_model and segments:
                logger.info("è¿›è¡Œå¼ºåˆ¶å¯¹é½...")
                result = whisperx.align(
                    result["segments"], 
                    self.align_model, 
                    self.metadata, 
                    audio, 
                    self.device, 
                    return_char_alignments=False
                )
                logger.info("âœ… å¼ºåˆ¶å¯¹é½å®Œæˆ")
            
            # 4. è¯´è¯äººè¯†åˆ«
            speaker_success = False
            if self.diarize_model:
                logger.info("å¼€å§‹è¯´è¯äººè¯†åˆ«...")
                try:
                    # ä½¿ç”¨æ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼
                    import torchaudio
                    waveform, sample_rate = torchaudio.load(audio_path)
                    diarization = self.diarize_model({"waveform": waveform, "sample_rate": sample_rate})
                    
                    # è½¬æ¢diarizationç»“æœä¸ºWhisperXæ ¼å¼
                    diarize_segments = []
                    for turn, _, speaker in diarization.itertracks(yield_label=True):
                        diarize_segments.append({
                            "start": turn.start,
                            "end": turn.end,
                            "speaker": speaker
                        })
                    
                    result = whisperx.assign_word_speakers(diarize_segments, result)
                    speaker_success = True
                    logger.info("âœ… è¯´è¯äººè¯†åˆ«å®Œæˆ")
                except Exception as e:
                    logger.warning(f"è¯´è¯äººè¯†åˆ«å¤±è´¥: {e}")
                    speaker_success = False
            else:
                speaker_success = False
            
            # 5. å¤„ç†å’Œæ ¼å¼åŒ–ç»“æœ
            processed_segments = self._format_results(
                result.get("segments", []), 
                dyad_id, 
                conversation_id, 
                speaker_success,
                duration
            )
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {processing_time:.1f}ç§’")
            
            # æ¸…ç†GPUå†…å­˜
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            
            return processed_segments
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _format_results(self, segments, dyad_id, conversation_id, speaker_success, total_duration):
        """æ ¼å¼åŒ–è½¬å½•ç»“æœ"""
        processed = []
        
        for i, seg in enumerate(segments):
            # å¤„ç†è¯´è¯äººä¿¡æ¯
            if speaker_success and "speaker" in seg:
                speaker_raw = seg["speaker"]
                # ç®€åŒ–è¯´è¯äººæ ‡è¯†
                if "SPEAKER_00" in speaker_raw:
                    speaker_name = "Speaker_A"
                elif "SPEAKER_01" in speaker_raw:
                    speaker_name = "Speaker_B"
                else:
                    speaker_name = f"Speaker_{speaker_raw.split('_')[-1]}"
            else:
                # ç®€å•äº¤æ›¿åˆ†é…
                speaker_name = f"Speaker_{'A' if i % 2 == 0 else 'B'}"
                speaker_raw = f"SPEAKER_{i % 2:02d}"
            
            # è·å–æ—¶é—´æˆ³
            start_seconds = seg.get('start', 0)
            end_seconds = seg.get('end', 0)
            duration = end_seconds - start_seconds
            
            # è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
            start_time = self._seconds_to_timestamp(start_seconds)
            finish_time = self._seconds_to_timestamp(end_seconds)
            
            # è·å–æ–‡æœ¬å’Œç½®ä¿¡åº¦
            text = seg.get('text', '').strip()
            confidence = seg.get('confidence', 0.0)
            
            # ç»Ÿè®¡è¯çº§ä¿¡æ¯
            words = seg.get('words', [])
            word_count = len(text.split()) if text else 0
            
            processed.append({
                'dyad': dyad_id,
                'conversation': conversation_id,
                'segment_id': i + 1,
                'start_time': start_time,  # æ–°æ ¼å¼ï¼šHH:MM:SS,mmm
                'finish_time': finish_time,  # æ”¹åä¸ºfinish_time
                'duration': round(duration, 3),
                'speaker': speaker_name,
                'speaker_raw': speaker_raw,
                'text': text,
                'confidence': round(confidence, 4),
                'word_count': word_count,
                'language': 'nl',  # è·å…°è¯­
                'model_used': 'large-v3',
                'device_used': self.device,
                'has_ai_speaker_detection': speaker_success
            })
        
        return processed
    
    def cleanup(self):
        """æ¸…ç†æ¨¡å‹å’Œå†…å­˜"""
        if self.model:
            del self.model
        if self.align_model:
            del self.align_model
        if self.diarize_model:
            del self.diarize_model
        
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info("å†…å­˜æ¸…ç†å®Œæˆ")
